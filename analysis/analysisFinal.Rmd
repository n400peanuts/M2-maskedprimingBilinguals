---
title: "Data analysis Morphological masked priming experiment on L1-ITA, L2-ENG bilingual speakers/readers"
subtitle: "Paper titled *Masked morphological priming tracks the development of a fully mature lexical system in L2* - Submitted to JML in June 2019"
author: 
- Eva Viviani
- Davide Crepaldi
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: 3
---

This script takes preprocessed data and produces all the analyses that are reported in the paper.


# clean WS, set WD and load data 
```{r clean environment, echo=FALSE}
rm(list = ls());
```


```{r set localgitdir, echo=FALSE}
#Set your local working directory. This should be (and is assumed to be in the rest of the code) the highest point in your local folder:
localGitDir <- 'C:/Users/eva_v/Documents/GitHub/M2-maskedprimingBilinguals'
setwd(localGitDir);
```

This script works on the outcome of preProcessing.R, which you can upload here:
```{r load data,  results="hide"}
read.table(paste(localGitDir, '/preprocessedData.txt', sep = ''), header = T, sep='\t', dec='.',
           colClasses=c("prime"="factor",
                        "target"="factor",
                        "lexicality"="factor",
                        "morphType"="factor",
                        "relatedness"="factor",
                        "aoa3.context"="factor",
                        "aoa4.contextMultling"="factor",
                        "aoa6.otherLang"="factor",
                        "language"="factor",
                         "gender"="factor",
                         "handedness"="factor"
                        )) -> masterFile; 
```


```{r load packages and create functions, message=FALSE, echo=FALSE, warning=FALSE}
CleanEnvir <- function(x) {rm(list=deparse(substitute(x)),envir=.GlobalEnv)}

library(tidyverse)
library(rms);
library(effects);
library(corrplot);
library(lme4);
library(ggpubr);
```

# Participants 
```{r n}
temp <- unique(masterFile[,c('subject','age','gender','handedness')]);
```

Number of participants: `r length(unique(temp$subject))`

Age, education and handedness: 
```{r summary}
summary(temp)
```
 
```{r, echo=FALSE}
rm(temp)
```

# Stimuli
```{r subset temp}
temp <- unique(masterFile[,c('target','prime','lexicality','morphType','relatedness','freqTarget','freqPrime','lengthTarget','lengthPrime','nTarget','nPrime','language')]); 
```

## Target features, ITA

Frequency of the targets, mean and sd
```{r freqtarget, echo=FALSE}
aggregate(freqTarget ~ morphType, FUN=mean, data=subset(temp, lexicality=='word' & language=='ita'));
aggregate(freqTarget ~ morphType, FUN=sd, data=subset(temp, lexicality=='word' & language=='ita'));
```

Length of the targets, mean and sd
```{r lengthtarget, echo=FALSE}
aggregate(lengthTarget ~ morphType, FUN=mean, data=subset(temp, lexicality=='word' & language=='ita'));
aggregate(lengthTarget ~ morphType, FUN=sd, data=subset(temp, lexicality=='word' & language=='ita'));
```

Coltheart's N, mean and sd
```{r nTarget, echo=FALSE}
aggregate(nTarget ~ morphType, FUN=mean, data=subset(temp, lexicality=='word' & language=='ita'));
aggregate(nTarget ~ morphType, FUN=sd, data=subset(temp, lexicality=='word' & language=='ita'));
```

## Prime features, ITA

Frequency of the primes, mean and sd
```{r freqprime, echo=FALSE}
aggregate(freqPrime ~ relatedness+morphType, FUN=mean, data=subset(temp, lexicality=='word' & language=='ita'));
aggregate(freqPrime ~ relatedness+morphType, FUN=sd, data=subset(temp, lexicality=='word' & language=='ita'));
```

Length of the primes, mean and sd
```{r lengthprime, echo=FALSE}
aggregate(lengthPrime ~ relatedness+morphType, FUN=mean, data=subset(temp, lexicality=='word' & language=='ita'));
aggregate(lengthPrime ~ relatedness+morphType, FUN=sd, data=subset(temp, lexicality=='word' & language=='ita'));
```

Coltheart's N, mean and sd
```{r nprime, echo=FALSE}
aggregate(nPrime ~ relatedness+morphType, FUN=mean, data=subset(temp, lexicality=='word' & language=='ita'));
aggregate(nPrime ~ relatedness+morphType, FUN=sd, data=subset(temp, lexicality=='word' & language=='ita'));
```
```{r , echo=FALSE}
rm(temp);
```

# Outliers trimming, ITA 

```{r subset masterFileIta}
subset(masterFile, language=="ita") -> masterFileIta;
```

The following code generates target and sbj means and SDs, and the outlier graphs in the file 'ita.jpg'. 
```{r run diagnostics}
sbj.id <- masterFileIta$subject;
acc <- masterFileIta$accuracy;
lexicality <- tolower(masterFileIta$lexicality);
target <- masterFileIta$target;
rt <- masterFileIta$rt;

source(paste(localGitDir, "/tools/diagnostics.R", sep='')); 
outlierGraphStore <- localGitDir;
diagnostics.f(rt = rt, acc = acc, sbj.id = sbj.id, target = target, lexicality = lexicality, paste(outlierGraphStore, "ita", sep=""));
rm(rt, target, lexicality, acc, sbj.id);
```

Based on the graphs in 'ita.jpg': we exclude **sbj 2** and **31** for an abnormal error rate on nonwords (<80%); and target words **GUANO**, **UGGIA** and **VELLO** with abnormally low accuracy (<60%). **sbj 15** is excluded because s/he reported having seen the primes.


Individual RTs distribution seems fine, but let's check the tails more carefully:
```{r tail plots, fig.height = 4, fig.width = 6, fig.align= "center" , echo=FALSE}
par(mfrow=c(1,2))
hist(masterFileIta$rt[masterFileIta$rt<500], breaks=seq(0,500,20), main = " ", xlab = " ");
hist(masterFileIta$rt[masterFileIta$rt>1500], breaks=seq(1500,3000,50), main = " ", xlab = " ");
par(mfrow=c(1,1))
```

Based on these graph we cut distributions at **2500ms** and **280ms**

```{r actual trimming}
dataItaAcc <- subset(masterFileIta, lexicality=="word");
dataItaTemp <- subset(dataItaAcc, accuracy==1);
dataIta <- subset(dataItaTemp, rt>280 & rt<2000 & subject!=15 & subject!=2 & subject!=31 & target!= "guano" & target!= "uggia" & target!= "vello");
```

- Number of datapoints trimmed: `r nrow(dataItaTemp)-nrow(dataIta)`
- Percentage of datapoints trimmed: `r round((nrow(dataItaTemp)-nrow(dataIta)) / nrow(dataItaTemp)*100, 2)`%
- Number of datapoints left: `r nrow(dataIta)`

Summary of the dataset trimmed:
```{r summary dataIta}
summary(dataIta)
```


# Outliers trimming, ENG

```{r subset masterfileEng}
subset(masterFile, language=="eng") -> masterFileEng;
```

The following code generates target and sbj means and SDs, and the outlier graphs in the file 'eng.jpg'

```{r diagnostic ENG}
sbj.id <- masterFileEng$subject;
acc <- masterFileEng$accuracy;
lexicality <- masterFileEng$lexicality;
lexicality <- tolower(masterFileEng$lexicality);
target <- masterFileEng$target;
rt <- masterFileEng$rt;

outlierGraphStore <- localGitDir;
diagnostics.f(rt = rt, acc = acc, sbj.id = sbj.id, target = target, lexicality = lexicality, paste(outlierGraphStore, "eng1", sep=""));
rm(rt, target, lexicality, acc, sbj.id);

```

**sbj 26** likely confused YES/NO buttons. Let's check the frequency effect, just to confirm:

```{r corr sbj 26, message=FALSE}
cor(masterFileEng[masterFileEng$subject==26 & masterFileEng$lexicality=='word', c('rt','freqTarget')], use = 'pairwise.complete.obs'); #it's unlikely s/he responded randomly. So, let's fix this:
masterFileEng$accuracy[masterFileEng$subject==26] <- car::recode(masterFileEng$accuracy[masterFileEng$subject==26], "1=0;0=1");

```

ok, we can now rerun diagnostics:

```{r diagnostic ENG round 2}
sbj.id <- masterFileEng$subject;
acc <- masterFileEng$accuracy;
lexicality <- masterFileEng$lexicality;
lexicality <- tolower(masterFileEng$lexicality);
target <- masterFileEng$target;
rt <- masterFileEng$rt;

diagnostics.f(rt = rt, acc = acc, sbj.id = sbj.id, target = target, lexicality = lexicality, paste(outlierGraphStore, "eng", sep=""));
rm(outlierGraphStore, rt, target, lexicality, acc, sbj.id);

```

Based on the graphs in 'eng.jpg': we exclude **sbj 22** for a very atypical performance (substantially below chance) on nonwords. We would exclude no target, even though some of them do elicit bad performance--the distribution is very continuous, no sign of glaring outliers. Plus, this is L2, so low performance is to be expected. **sbj 15** and **43** reported having seen the primes.


Individual RTs distribution seems fine, but let's check the tails more carefully:

```{r check tails distribution, fig.height = 4, fig.width = 6, fig.align= "center", echo=FALSE }
par(mfrow=c(1,2))
hist(masterFileEng$rt[masterFileEng$rt<500], breaks=seq(0,500,20), main = "", xlab = ""); 
hist(masterFileEng$rt[masterFileEng$rt>1500], breaks=seq(1500,5500,50), main = "", xlab = "");
par(mfrow=c(1,1))
```

- First graph: very continuous towards zero, plus a few zeroes. Anyway, deflection in the curve around 300ms, so let's cut there.
- Second graph:  clear outliers over 2000ms

```{r trim eng}
dataEngAcc <- subset(masterFileEng, lexicality=="word");
dataEngTemp <- subset(dataEngAcc, accuracy==1);
dataEng <- subset(dataEngTemp, rt>300 & rt<2000 & subject!=15 & subject!=22 & subject!=43);

```

- Number of datapoints trimmed: `r nrow(dataEngTemp)-nrow(dataEng)`
- Percentage of datapoints trimmed: `r round((nrow(dataEngTemp)-nrow(dataEng)) / nrow(dataEngTemp)*100, 2)`%
- Number of datapoints left: `r nrow(dataEng)`


Summary of the trimmed dataset:
```{r summary dataEng}
summary(dataEng);
```

```{r, echo=FALSE}
#clean up the workspace
rm(masterFileIta, masterFileEng, diagnostics.f, sbj.diagnostics, target.diagnostics);
```

# Descriptive statistics

- Mean accuracy ITA: `r round(mean(dataItaAcc$accuracy),2)*100`%
- Mean RT ITA: `r round(mean(dataIta$rt),2)` ms
- Mean accuracy ENG: `r round(mean(dataEngAcc$accuracy),2)*100`% 
- Mean RT ENG: `r round(mean(dataEng$rt),2)` ms

Mean RT and sd by relatedness and morphtype - ITA dataset:
```{r, echo=FALSE}
aggregate(rt ~ relatedness + morphType, FUN=mean, data=dataIta);
```

```{r, echo=FALSE}
aggregate(rt ~ relatedness + morphType, FUN=sd, data=dataIta);
```

Mean RT and sd  by relatedness and morphtype - ENG dataset:
```{r, echo=FALSE}
aggregate(rt ~ relatedness + morphType, FUN=mean, data=dataEng);
```

```{r, echo=FALSE}
aggregate(rt ~ relatedness + morphType, FUN=sd, data=dataEng);
```

# Modelling

```{r load models, echo=FALSE}
#GLMMs may take a lot of time to run, depending on the machine and the dataset. We have saved the models that successfully converged on our machines, so that you can just upload them, instead of re-fitting: 

df <- list.files(paste(localGitDir, "/analysis/GLMMs/", sep = "")); 

for (i in 1:length(df)){
  gsub(".rds$", "", df[i]) -> id
  assign(id, data.frame())
  readRDS(paste(localGitDir, "/analysis/GLMMs/", df[i],sep = "")) -> temp 
  assign(paste0(id), temp)
};
rm(temp)
```

The code through which we generated the dataset is reported here below.

## Italian, L1
Constrasts:
```{r Set level of the contrasts, echo=FALSE}
dataIta$morphType <- relevel(dataIta$morphType, "or");
contrasts(dataIta$relatedness);
contrasts(dataIta$morphType);
```

```{r itaglmer0, eval = FALSE}
itaglmer0<- glmer(rt ~ 1 + (1|subject) + (1|target), data= dataIta, family=Gamma(link="identity"));
```

```{r itaglmer1, eval = FALSE}
itaglmer1<- glmer(rt ~ trialCount + rotation + (1|subject) + (1|target), data= dataIta, family=Gamma(link="identity"));
```
```{r comparison between itaglmer0 and itaglmer1}
anova(itaglmer0, itaglmer1);
```

no effect of rotation here

```{r itaglmer1a, eval = FALSE}
itaglmer1a<- glmer(rt ~ freqTarget + lengthTarget + nTarget + (1|subject) + (1|target), data= dataIta, family=Gamma(link="identity"));
```

```{r comparison between itaglmer0 and itaglmer1a}
anova(itaglmer0, itaglmer1a);
```

strong improvement in GoF

```{r anova itaglmer1a}
car::Anova(itaglmer1a)
```

To which only freq seems to contribute. We introduce the variables of interest now:

```{r itaglmer2, eval = FALSE}
itaglmer2<- glmer(rt ~ relatedness * morphType + freqTarget + (1|subject) + (1|target), data= dataIta, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
```

```{r anova and summary itaglmer2}
summary(itaglmer2)-> modelsum2
car::Anova(itaglmer2)-> itaglmer2.anova
itaglmer2.anova
```

With "orthographic" and "unrelated/ctrl" as base contrast:
```{r table of the summary}
knitr::kable(round(modelsum2$coefficients, 4))
```

Following the suggestion of a reviewer, we check here that the results remain the same when we exclude the participants who were also excluded in the L2 analysis (i.e., when the set of participants is identical in in the L1 and L2 experiments);
```{r itaglmer2check, eval=FALSE}
itaglmer2check <- glmer(rt ~ relatedness * morphType + freqTarget + (1|subject) + (1|target), data=subset(dataIta, subject!=15 & subject!=22 & subject!=43), family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
```

```{r}
knitr::kable(round(summary(itaglmer2check)$coefficients,6));
car::Anova(itaglmer2check);
```


Let's relevel for transparent versus opaque condition:

```{r relevel again by op}
dataIta$morphType <- relevel(dataIta$morphType, "op");
```

```{r itaglmer2c, eval=FALSE}
itaglmer2c<- glmer(rt ~ relatedness * morphType + freqTarget + (1|subject) + (1|target), data= dataIta, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
```

```{r anova and summary itaglmer2c}
summary(itaglmer2c)->modelsum2c
``` 

```{r}
knitr::kable(round(modelsum2c$coefficients, 4))
```

### Summary of the results for ITA dataset

The model itaglmer2 shows a significant interaction between relatedness and morphtype [$\chi^2$ = `r round(itaglmer2.anova$Chisq[4], 2)`, $Df$ = `r itaglmer2.anova$Df[4]`, *p*  $<$ 0.0001], such that among related trials there were faster reaction times to orthographic trials compared to opaque trials [$\beta$ = `r round(modelsum2$coefficients, 2)[6,1]`, $Std.Error$ = `r round(modelsum2$coefficients[6,2],2)`, $t$ = `r round(modelsum2$coefficients[6,3],2)`, *p*  $<$ 0.0001], and to transparent trials [$\beta$ = `r round(modelsum2$coefficients, 2)[7,1]`, $Std.Error$ = `r round(modelsum2$coefficients[7,2],2)`, $t$ = `r round(modelsum2$coefficients[7,3],2)`, *p*  $<$ 0.0001]. By setting the base contrast to opaque trials, itaglmer2c shows that opaque trials are faster than transparent trials [$\beta$ = `r round(modelsum2c$coefficients, 2)[7,1]`, $Std.Error$ = `r round(modelsum2c$coefficients[7,2],2)`, $t$ = `r round(modelsum2c$coefficients[7,3],2)`, *p*  $<$ 0.0001]. 

## English, L2

Set contrasts:
```{r set contrast eng, echo=FALSE}
dataEng$morphType <- relevel(dataEng$morphType, "or");
contrasts(dataEng$relatedness);
contrasts(dataEng$morphType);
```


```{r engglmer0, eval=FALSE}
engglmer0 <- glmer(rt ~ 1+ (1|subject) + (1|target), data = dataEng, 
      family=Gamma(link="identity")); 
```

```{r engglmer1, eval=FALSE}
engglmer1 <- glmer(rt ~ trialCount + rotation+ (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity")); 
```

```{r}
anova(engglmer0, engglmer1); 

```

Again like in the ITA dataset, no effect of rotation here.

```{r engglmer1c, eval=FALSE}
engglmer1c <- glmer(rt ~ freqTarget + lengthTarget + nTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
```

```{r}
anova(engglmer0, engglmer1c); 
```

strong improvement in GoF

```{r engglmer1c anova}
car::Anova(engglmer1c)
```

Frequency and length contribute.

We introduce our variable of interest now:

```{r engglmer2, eval=FALSE}
engglmer2 <- glmer(rt ~ relatedness * morphType + freqTarget + lengthTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
```

```{r}
engglmer2.anova <- car::Anova(engglmer2)
engglmer2.modelsum <- summary(engglmer2)
engglmer2.anova
```

Summary of the engglmer2 model:
```{r}
knitr::kable(round(engglmer2.modelsum$coefficients, 4))
```


We set the base contrast on the opaque condition and re-run the model:
```{r reset contrast opaque dataEng}
dataEng$morphType <- relevel(dataEng$morphType, "op");
```

```{r engglmer2c, eval=FALSE}
engglmer2c <- glmer(rt ~ relatedness * morphType + freqTarget + lengthTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
```

```{r}
engglmer2c.modelsum <- summary(engglmer2c)
```

```{r}
knitr::kable(round(engglmer2c.modelsum$coefficients, 4))
```


### Summary of the results for ENG dataset

The model engglmer2 shows a significant interaction between relatedness and morphtype [$\chi^2$ = `r round(engglmer2.anova$Chisq[5], 2)`, $Df$ = `r engglmer2.anova$Df[5]`, *p*  $<$ 0.0001], such that among related trials there were faster reaction times to orthographic trials compared to opaque trials, although only marginally significant [$\beta$ = `r round(engglmer2.modelsum$coefficients, 2)[7,1]`, $Std.Error$ = `r round(engglmer2.modelsum$coefficients[7,2],2)`, $t$ = `r round(engglmer2.modelsum$coefficients[7,3],2)`, *p*  = `r round(engglmer2.modelsum$coefficients[7,4],4)`], and to transparent trials [$\beta$ = `r round(engglmer2.modelsum$coefficients, 2)[8,1]`, $Std.Error$ = `r round(engglmer2.modelsum$coefficients[8,2],2)`, $t$ = `r round(engglmer2.modelsum$coefficients[8,3],2)`, *p*  $<$ 0.0001]. By setting the base contrast to opaque trials, engglmer2c shows that opaque trials are faster than transparent trials [$\beta$ = `r round(engglmer2c.modelsum$coefficients, 2)[8,1]`, $Std.Error$ = `r round(engglmer2c.modelsum$coefficients[8,2],2)`, $t$ = `r round(engglmer2c.modelsum$coefficients[8,3],2)`, *p*  $<$ 0.0002]. 


# Cross language interaction 

```{r rbind of dataIta and dataENG}
rbind(dataEng, dataIta) -> crossExp
summary(crossExp)
```

Setting back the baseline to the orthographic condition:
```{r}
crossExp$morphType <- relevel(crossExp$morphType, "or");
```

```{r crossglmer run, eval=FALSE}
crossglmer <- glmer(rt ~ relatedness * morphType * language + freqTarget + lengthTarget + (1|subject) + (1|target), data = crossExp, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5))); 
```

```{r}
crossglmer.anova <- car::Anova(crossglmer)
```
We check here that results remain the same when we exclude from L1 also those participants who were excluded in L2 (i.e., when the exact same set of participants is considered in the L1 and L2 datasets);
```{r crossglmerCheck, eval=FALSE}
crossglmerCheck <- glmer(rt ~ relatedness * morphType * language + freqTarget + lengthTarget + (1|subject) + (1|target), data = subset(crossExp, subject!=15 & subject!=22 & subject!=43), family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5))); 
```

```{r}
knitr::kable(round(summary(crossglmerCheck)$coefficients,3));
car::Anova(crossglmerCheck);
```


## Summary of the results for the cross language interaction (ITA-ENG)
The cross–language analysis confirms that the priming pattern across conditions is different in L1 and L2, as attested by the significant interaction between prime relatedness, morphological type and language, [$\chi^2$ = `r round(crossglmer.anova$Chisq[9], 2)`, $Df$ = `r crossglmer.anova$Df[9]`, *p*  $<$ 0.0001].

# Proficiency scores, correlation, reliability and distribution 
Create a dataframe with one row per participant:

```{r subset pptFeatures}
pptFeatures <- unique(dataEng[,c('subject','age','gender','handedness','rotation','phonemicFluency', 'phonemicComprehension','morphComprehension','spelling','readingComprehension','vocabulary','oralComprehension','aoa1.Aoa', 'aoa2.usage', 'aoa3.context','aoa4.contextMultling','aoa5.selfRatedProf','aoa6.otherLang')]);
summary(pptFeatures)
```

```{r summary of pptFeatures correlations, warning=FALSE, message=FALSE}
knitr::kable(round(cor(pptFeatures[,c(6:12)], use='pairwise.complete.obs'), digits=2))
library(languageR);
collin.fnc(pptFeatures[,c(6:12)])$cnumber;
```

## Principal Component Analysis (PCA)

To confirm that the structure of our battery of tests is not further reducible into a smaller number of independent predictors, we ran a Principal Component Analysis with a Varimax rotation:

```{r, echo=FALSE}
pptFeatures<-pptFeatures %>%
  rename(morphAwareness = morphComprehension,
         phonemicDiscrimination = phonemicComprehension)
```


```{r, message=FALSE, warning=FALSE}
library(psych);
pcaVarimax <- principal(pptFeatures[,c(6:12)], nfactors=7, rotate='varimax'); 
pcaVarimax;
pcaVarimax$loadings;
```
Two important results emerge here. First, the seven PCs explain a very similar amount of variance; and second, each of them loads very heavily on one specific proficiency index. This indicates that our seven variables constitute a minimal set of interpretable predictors.

Let's represent this graphically:
```{r PCA plot, fig.height = 7, fig.width = 6, fig.align= "center", echo=FALSE}
par(fig=c(0,1,0,0.7));
corrplot(pcaVarimax$loadings[1:7,1:7], method='color', tl.col='black');
par(fig=c(.278,1,.54,1), new=TRUE);
plot(1:7*.93, pcaVarimax$Vaccounted[2,], bty='n', xlab='', ylab='Proportion of variance', type='b', pch=19, ylim=c(.10,.18), axes=F, xlim=c(1,7));
axis(2);
axis(1, at=1:7*.93, labels=rep('',7));
```

## Distribution of accuracy as a function of proficiency 

Reviewers asked us the distribution of accuracy obtained by each participant in L2 against the standardized score in each proficiency subtest.

```{r plot correlation between accuracy and proficiency, echo=FALSE}
meanAcc_bysubj<-aggregate(accuracy ~ subject, data = dataEngAcc[dataEngAcc$subject!=15 & dataEngAcc$subject!=22 & dataEngAcc$subject!=43,], mean)

acc_by_proficiency<-data.frame(
  subject= rep(pptFeatures$subject,7),
  accuracy = rep(meanAcc_bysubj$accuracy,7),
  proficiency = c(rep("phonemicFluency",78),rep("phonemicDiscrimination",78),
                  rep("morphAwareness",78), rep("spelling",78),
                  rep("readingComprehension",78),rep("vocabulary",78),
                  rep("oralComprehension",78)),
  z_scoreProficiency = c(scale(pptFeatures$phonemicFluency),scale(pptFeatures$phonemicComprehension),
                         scale(pptFeatures$morphComprehension),scale(pptFeatures$spelling),
                         scale(pptFeatures$readingComprehension),scale(pptFeatures$vocabulary),
                         scale(pptFeatures$oralComprehension))
)


scatterAcc_by_proficiency<-ggscatter(acc_by_proficiency, x = "z_scoreProficiency", y = "accuracy",
          facet.by = "proficiency",
          ylab = "average accuracy",
          xlab = "Z scores",
          color = "black", shape = 21, size = 3,
          ylim = c(0.3,1),
          )+ stat_cor(aes(label = ..r.label..), label.x = .8, label.y = .4)
scatterAcc_by_proficiency
```
```{r, echo=FALSE}
rm(acc_by_proficiency,scatterAcc_by_proficiency)
```


# Proficiency modelling 
Set base contrasts to orthographic
```{r base contrasts of dataEng}
dataEng$morphType <- relevel(dataEng$morphType, "or");
```

This model establishes the baseline model, with no proficiency score:

```{r proficiencyglmer0, eval=FALSE}
proficiencyglmer0 <- glmer(rt ~ relatedness * morphType  + freqTarget + lengthTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
```

Now we test whether each individual proficiency score guarantees a better fit overall:

## Phonemic fluency
```{r proficiencyglmer1, eval=FALSE}
proficiencyglmer1 <- glmer(rt ~ relatedness  * morphType * phonemicFluency + lengthTarget + freqTarget + (1|subject) 
      + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
anova(proficiencyglmer0, proficiencyglmer1);
```

## Phonemic comprehension
```{r proficiencyglmer2, eval=FALSE}
proficiencyglmer2<- glmer(rt ~ relatedness  * morphType * phonemicComprehension + lengthTarget + freqTarget + (1|subject) 
         + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa"), optCtrl=list(maxfun=2e5)));
anova(proficiencyglmer0, proficiencyglmer2);
```

## Morphological awareness
```{r proficiencyglmer3, eval=FALSE}
proficiencyglmer3<- glmer(rt ~ relatedness  * morphType * morphComprehension + lengthTarget + freqTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5))); 
anova(proficiencyglmer0, proficiencyglmer3);
```

## Spelling
```{r proficiencyglmer4, eval=FALSE}
proficiencyglmer4<- glmer(rt ~ relatedness  * morphType * spelling + lengthTarget + freqTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
anova(proficiencyglmer0, proficiencyglmer4);
```

## Reading comprehension
```{r proficiencyglmer5, eval=FALSE}
proficiencyglmer5<- glmer(rt ~ relatedness  * morphType * readingComprehension + lengthTarget + freqTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
anova(proficiencyglmer0, proficiencyglmer5);
```

## Vocabulary
```{r proficiencyglmer6, eval=FALSE}
proficiencyglmer6<- glmer(rt ~ relatedness  * morphType * vocabulary + lengthTarget + freqTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
anova(proficiencyglmer0, proficiencyglmer6);
```

## Oral comprehension
```{r proficiencyglmer7, eval=FALSE}
proficiencyglmer7<- glmer(rt ~ relatedness  * morphType * oralComprehension + lengthTarget + freqTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
anova(proficiencyglmer0, proficiencyglmer7);
```

All tests seem to increase the goodness of fit of the base model proficiencyglmer0.

# Does proficiency specifically interact with priming?

We're going to inspect the anova of each model first, so that we understand which metric we can focus on. 
```{r anovas}
car::Anova(proficiencyglmer1) -> proficiencyglmer1.anova
car::Anova(proficiencyglmer2) -> proficiencyglmer2.anova
car::Anova(proficiencyglmer3) -> proficiencyglmer3.anova
car::Anova(proficiencyglmer4) -> proficiencyglmer4.anova
car::Anova(proficiencyglmer5) -> proficiencyglmer5.anova
car::Anova(proficiencyglmer6) -> proficiencyglmer6.anova
car::Anova(proficiencyglmer7) -> proficiencyglmer7.anova
proficiencyglmer1.anova
proficiencyglmer2.anova
proficiencyglmer3.anova
proficiencyglmer4.anova
proficiencyglmer5.anova
proficiencyglmer6.anova
proficiencyglmer7.anova
```
Phonemic discrimination, morphological awareness and vocabulary are clearly significant; we will explore these effects in depth below. Oral comprehension and spelling are clearly non-significant, so we'll just drop them. Phonemic fluency and reading comprehension do not pass the significance test; therefore, coherently with our rather conservative approach, we will not discuss them in the paper. However, they are relatively close to significance, so we will explore them slightly more in depth in this script -- the user will decide for her/himself what to do of this additional evidence.

## The fully significant effects: phonemic discrimination, morphological awareness and vocabulary.

```{r phonemicdiscrimination plot 2, fig.height = 4, fig.width = 10, fig.align= "center", echo=FALSE}
temp <- data.frame(effect('relatedness:morphType:phonemicComprehension', proficiencyglmer2, se=list(level=.95), xlevels=list(phonemicComprehension=quantile(dataEng$phonemicComprehension, probs=c(.05,.50,.95)))));
revalue(temp$relatedness, c("rel"="Related"))-> temp$relatedness;
revalue(temp$relatedness, c("ctrl"="Unrelated"))-> temp$relatedness;

phonComprehension_names <- c(
  "5" = "Low phonDiscrimination",
  "9" = "Medium phonDiscrimination",
  "12" = "High phonDiscrimination");

ggplot(data = temp, aes(x=relatedness, y=fit, group=morphType)) + 
  geom_point(size = 2, position = position_dodge(width = 0.25)) +
  geom_line(aes(linetype=morphType), position = position_dodge(width = 0.25)) + 
  scale_linetype_manual(values=c("dotted", "dashed", "solid")) +
  theme_bw() + 
  theme(panel.grid.major = element_blank()) +
  ylab('RTs (ms)') + xlab('') + 
  theme(axis.text.y = element_text(angle = 00, hjust = 1, size=8, colour = 'black'))+
  theme(axis.text.x = element_text(size=13, colour = 'black'))+
  geom_pointrange(aes(ymin = lower, ymax = upper), position = position_dodge(width = 0.25)) +
  facet_grid(~ phonemicComprehension, 
             labeller = labeller(phonemicComprehension = as_labeller(phonComprehension_names))) +
  theme(strip.text = element_text(size=12)) + 
  theme(legend.position="none");
```

It seems that opaque and transparent priming are both strong across the phonemic discrimination scale, while orthographic priming shrinks to zero with growing phonemic discrimination. This is confirmed by the model parameters:
```{r}
knitr::kable(round(summary(proficiencyglmer2)$coefficients,3));
```

But we can see better here, where the orthographic condition is not the baseline and therefore it is explicitly pitted against one of the others (opaque, in this case):

```{r}
dataEng$morphType <- relevel(dataEng$morphType, "op");
```

```{r, eval=FALSE}
proficiencyglmer2b <- glmer(rt ~ relatedness  * morphType * phonemicComprehension + lengthTarget + freqTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa"), optCtrl=list(maxfun=2e5)));
```

```{r}
knitr::kable(round(summary(proficiencyglmer2b)$coefficients,3));
```

A similar pattern emerges with vocabulary, in particular for what concerns the orthographic and transparent conditions:
```{r vocabulary plot, fig.height = 4, fig.width = 10, fig.align= "center", echo=FALSE}
temp <- data.frame(effect('relatedness:morphType:vocabulary', proficiencyglmer6, se=list(level=.95), xlevels=list(vocabulary=quantile(dataEng$vocabulary, probs=c(.05,.50,.95)))));
revalue(temp$relatedness, c("rel"="Related"))-> temp$relatedness;
revalue(temp$relatedness, c("ctrl"="Unrelated"))-> temp$relatedness;

vocabulary_names <- c(
  "11" = "Low vocabulary",
  "16" = "Medium vocabulary",
  "19" = "High vocabulary");

ggplot(data = temp, aes(x=relatedness, y=fit, group=morphType)) + 
  geom_point(size = 2, position = position_dodge(width = 0.25)) +
  geom_line(aes(linetype=morphType), position = position_dodge(width = 0.25)) + 
  scale_linetype_manual(values=c("dotted", "dashed", "solid")) +
  theme_bw() + 
  theme(panel.grid.major = element_blank()) +
  ylab('RTs (ms)') + xlab('') + 
  theme(axis.text.y = element_text(angle = 00, hjust = 1, size=8, colour = 'black'))+
  theme(axis.text.x = element_text(size=13, colour = 'black'))+
  geom_pointrange(aes(ymin = lower, ymax = upper), position = position_dodge(width = 0.25)) +
  facet_grid(~ vocabulary, 
             labeller = labeller(vocabulary = as_labeller(vocabulary_names))) +
  theme(strip.text = element_text(size=12)) + 
  theme(legend.position="none");
```
```{r}
round(summary(proficiencyglmer6)$coefficients,3);
```

This time, though, also the opaque condition seems to flatten with growing proficiency, as we can see here:
```{r, eval=FALSE}
proficiencyglmer6b<- glmer(rt ~ relatedness  * morphType * vocabulary + lengthTarget + freqTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
```

```{r}
round(summary(proficiencyglmer6b)$coefficients,3);

```

Essentially: both orthographic and opaque priming shrinks with growing vocabulary, as compared to transparent priming, which remains solid across the board. Opaque priming, however, flattens less than orthographic priming, that is, there's still a genuine morphological effect here, although morpho-orthographic differs from morpho-semantic. From a different perspective, the pattern with orthographic and transparent priming confirms what we see through phonemic awareness; the opaque priming pattern is different from phonemic awareness, instead.

Morphological awareness is a bit different, instead:
```{r morphological awareness plot, fig.height = 4, fig.width = 10, fig.align= "center", echo=FALSE}
temp <- data.frame(effect('relatedness:morphType:morphComprehension', proficiencyglmer3, se=list(level=.95), xlevels=list(morphComprehension=quantile(dataEng$morphComprehension, probs=c(.05,.50,.95)))));
revalue(temp$relatedness, c("rel"="Related"))-> temp$relatedness;
revalue(temp$relatedness, c("ctrl"="Unrelated"))-> temp$relatedness;

morphComprehension_names <- c(
  "6" = "Low morphAwareness",
  "9" = "Medium morphAwareness",
  "10" = "High morphAwareness");

ggplot(data = temp, aes(x=relatedness, y=fit, group=morphType)) + 
  geom_point(size = 2, position = position_dodge(width = 0.25)) +
  geom_line(aes(linetype=morphType), position = position_dodge(width = 0.25)) + 
  scale_linetype_manual(values=c("dotted", "dashed", "solid")) +
  theme_bw() + 
  theme(panel.grid.major = element_blank()) +
  ylab('RTs (ms)') + xlab('') + 
  theme(axis.text.y = element_text(angle = 00, hjust = 1, size=8, colour = 'black'))+
  theme(axis.text.x = element_text(size=13, colour = 'black'))+
  geom_pointrange(aes(ymin = lower, ymax = upper), position = position_dodge(width = 0.25)) +
  facet_grid(~ morphComprehension, 
             labeller = labeller(morphComprehension = as_labeller(morphComprehension_names))) +
  theme(strip.text = element_text(size=12)) + 
  theme(legend.position="none");
```
```{r}
knitr::kable(round(summary(proficiencyglmer3)$coefficients,3));
```


```{r summary proficiencyglmer3b, eval=FALSE}
proficiencyglmer3b <- glmer(rt ~ relatedness  * morphType * morphComprehension + lengthTarget + freqTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5))); 
```

```{r}
knitr::kable(round(summary(proficiencyglmer3b)$coefficients,3));
```

Indeed, opaque differs from both orthographic and transparent, but transparent and orthographic do NOT differ. Quite different from what we observe anove for phonemic discrimination and vocabulary.

## The quasi-significant effects: phonemic fluency and reading comprehension
```{r}
knitr::kable(round(summary(proficiencyglmer1)$coefficients,3));
knitr::kable(round(summary(proficiencyglmer5)$coefficients,3));
```


```{r anova and summary proficiencyglmer1b, eval=FALSE}

proficiencyglmer1b <- glmer(rt ~ relatedness  * morphType * phonemicFluency + lengthTarget + freqTarget + (1|subject) 
      + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));

proficiencyglmer5b<- glmer(rt ~ relatedness  * morphType * readingComprehension + lengthTarget + freqTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5))); 
```

```{r}
knitr::kable(round(summary(proficiencyglmer1b)$coefficients,3));
knitr::kable(round(summary(proficiencyglmer5b)$coefficients,3));

```

Both quasi-significances seem to come from specific contrasts between morphological conditions. For phonemic fluency, transparent primes seem to differ from opaque primes (but no other contrast is significant). For reading comprehension, transparent primes seem to differ from orthographic primes (but no other contrast is significant). Also, the contrast between transparent and opaque primes in the phonemic fluency analysis seems more convincing statistically (t = -2.66) than the contrast between transparent and orthographic primes in the reading comprehension analysis (t = -2.14). 

As a bonus analysis, we exposed the coefficients for the proficiency interactions *for each morphological condition independently*. That is, we're not comparing anymore conditions against each others; we evaluate here the change in the priming slope for each condition.
```{r proficiencyglmer2_estimates, eval=FALSE}
proficiencyglmer2_estimate <- 
  glmer(rt ~  relatedness + morphType + phonemicComprehension + relatedness:morphType + 
          morphType:phonemicComprehension + relatedness:morphType:phonemicComprehension + 
          lengthTarget + freqTarget + (1|subject) + (1|target),  data = dataEng, family=Gamma(link="identity"), 
                                 control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));

```

```{r}
knitr::kable(round(summary(proficiencyglmer2_estimate)$coefficients,4))

```

```{r proficiencyglmer3_estimate, eval=FALSE}
proficiencyglmer3_estimate <- 
  glmer(rt ~  relatedness + morphType + morphComprehension + relatedness:morphType + 
          morphType:morphComprehension + relatedness:morphType:morphComprehension + 
          lengthTarget + freqTarget + (1|subject) + (1|target),  data = dataEng, family=Gamma(link="identity"), 
                                 control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));

```

```{r}
knitr::kable(round(summary(proficiencyglmer3_estimate)$coefficients,4))

```

```{r proficiencyglmer6_estimate, eval=FALSE}
proficiencyglmer6_estimate<- glmer(rt ~ relatedness  + morphType + vocabulary + relatedness:morphType + morphType:vocabulary + relatedness:morphType:vocabulary + lengthTarget + freqTarget + (1|subject) + (1|target), 
                                data = dataEng, family=Gamma(link="identity"), 
                                control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));

```

```{r}
knitr::kable(round(summary(proficiencyglmer6_estimate)$coefficients,4))

```


## Jackknife
we further assessed the reliability of the proficiency results via a \emph{jackknife} procedure \citep{ang1998use}---we repeatedly fitted the models described above to a subsample of the original observation set and checked that the model estimates remained fairly stable. These additional analyses fully confirm the pattern of results illustrated above, as shown in the figure below:

```{r jaccknife plots, echo=FALSE}
group3df<-read.csv(paste0(localGitDir,"/analysis/jaccknife/bootstrap_groupAnalysis.csv"))
phonDiscr<-read.csv(paste0(localGitDir,"/analysis/jaccknife/bootstrap_phonDiscr.csv"))
morphAware<-read.csv(paste0(localGitDir,"/analysis/jaccknife/bootstrap_morphAware.csv"))
vocjack<-read.csv(paste0(localGitDir,"/analysis/jaccknife/bootstrap_voc.csv"))

par(mfrow=c(2,2));
par(mar=c(1,1,3,.5)+.1);

plot(c(1,2), c(8,-31), type='n', xlim=c(1,2), ylim=c(-35, 10), axes=F, main='Group-level analysis', xlab='', ylab='Beta');
axis(1, at=c(1.25,1.75), labels=c('Opaque-Orthographic', 'Transparent-Orthographic'));
axis(2);
abline(h=0, lty=2, lwd=2);
lowerLim1 <- quantile(group3df$relatednessrel.morphTypeop, probs=.05);
upperLim1 <- quantile(group3df$relatednessrel.morphTypeop, probs=.95);
lowerLim2 <- quantile(group3df$relatednessrel.morphTypetr, probs=.05);
upperLim2 <- quantile(group3df$relatednessrel.morphTypetr, probs=.95);
polygon(c(1.10,1.10,1.40,1.40), c(lowerLim1, upperLim1, upperLim1, lowerLim1));
polygon(c(1.60,1.60,1.90,1.90), c(lowerLim2, upperLim2, upperLim2, lowerLim2));
lines(c(1.10,1.40), rep(median(group3df$relatednessrel.morphTypeop),2), lwd=2);
lines(c(1.60,1.90), rep(median(group3df$relatednessrel.morphTypetr),2), lwd=2);
points(1.25, fixef(engglmer2)[7], col=rgb(1,0,0,.8), pch=19, cex=2);
points(1.75, fixef(engglmer2)[8], col=rgb(1,0,0,.8), pch=19, cex=2);
text(1.25, 5, 'p=.059');
text(1.75, -7.5, 'p<.001');

plot(c(1,2), c(0,-8), type='n', xlim=c(1,2), ylim=c(-10.5, 1), axes=F, main='Phonemic Discrimination', xlab='', ylab='Beta');
axis(1, at=c(1.25,1.75), labels=c('Opaque-Orthographic', 'Transparent-Orthographic'));
axis(2);
abline(h=0, lty=2, lwd=2);
lowerLim1 <- quantile(phonDiscr$relatednessrel.morphTypeop.phonemicComprehension, probs=.05);
upperLim1 <- quantile(phonDiscr$relatednessrel.morphTypeop.phonemicComprehension, probs=.95);
lowerLim2 <- quantile(phonDiscr$relatednessrel.morphTypetr.phonemicComprehension, probs=.05);
upperLim2 <- quantile(phonDiscr$relatednessrel.morphTypetr.phonemicComprehension, probs=.95);
polygon(c(1.10,1.10,1.40,1.40), c(lowerLim1, upperLim1, upperLim1, lowerLim1));
polygon(c(1.60,1.60,1.90,1.90), c(lowerLim2, upperLim2, upperLim2, lowerLim2));
lines(c(1.10,1.40), rep(median(phonDiscr$relatednessrel.morphTypeop.phonemicComprehension),2), lwd=2);
lines(c(1.60,1.90), rep(median(phonDiscr$relatednessrel.morphTypetr.phonemicComprehension),2), lwd=2);
points(1.25, fixef(proficiencyglmer2)[13], col=rgb(1,0,0,.8), pch=19, cex=2);
points(1.75, fixef(proficiencyglmer2)[14], col=rgb(1,0,0,.8), pch=19, cex=2);
text(1.25, upperLim1+1, 'p<.001');
text(1.75, upperLim2+1, 'p<.001');

plot(c(1,2), c(0,-8), type='n', xlim=c(1,2), ylim=c(-5.5, 10), axes=F, main='Morphological Awareness', xlab='', ylab='Beta');
axis(1, at=c(1.25,1.75), labels=c('Opaque-Orthographic', 'Transparent-Orthographic'));
axis(2);
abline(h=0, lty=2, lwd=2);
lowerLim1 <- quantile(morphAware$relatednessrel.morphTypeop.morphComprehension, probs=.05);
upperLim1 <- quantile(morphAware$relatednessrel.morphTypeop.morphComprehension, probs=.95);
lowerLim2 <- quantile(morphAware$relatednessrel.morphTypetr.morphComprehension, probs=.05);
upperLim2 <- quantile(morphAware$relatednessrel.morphTypetr.morphComprehension, probs=.95);
polygon(c(1.10,1.10,1.40,1.40), c(lowerLim1, upperLim1, upperLim1, lowerLim1));
polygon(c(1.60,1.60,1.90,1.90), c(lowerLim2, upperLim2, upperLim2, lowerLim2));
lines(c(1.10,1.40), rep(median(morphAware$relatednessrel.morphTypeop.morphComprehension)-.03,2), lwd=2);
lines(c(1.60,1.90), rep(median(morphAware$relatednessrel.morphTypetr.morphComprehension),2), lwd=2);
points(1.25, fixef(proficiencyglmer3)[13], col=rgb(1,0,0,.8), pch=19, cex=2);
points(1.75, fixef(proficiencyglmer3)[14], col=rgb(1,0,0,.8), pch=19, cex=2);
text(1.25, upperLim1+1, 'p<.001');
text(1.75, upperLim2+1, 'p=.311');

plot(c(1,2), c(0,-8), type='n', xlim=c(1,2), ylim=c(-7.5, 3), axes=F, main='vocabulary', xlab='', ylab='Beta');
axis(1, at=c(1.25,1.75), labels=c('Opaque-Orthographic', 'Transparent-Orthographic'));
axis(2);
abline(h=0, lty=2, lwd=2);
lowerLim1 <- quantile(vocjack$relatednessrel.morphTypeop.vocjack, probs=.05);
upperLim1 <- quantile(vocjack$relatednessrel.morphTypeop.vocjack, probs=.95);
lowerLim2 <- quantile(vocjack$relatednessrel.morphTypetr.vocjack, probs=.05);
upperLim2 <- quantile(vocjack$relatednessrel.morphTypetr.vocjack, probs=.95);
polygon(c(1.10,1.10,1.40,1.40), c(lowerLim1, upperLim1, upperLim1, lowerLim1));
polygon(c(1.60,1.60,1.90,1.90), c(lowerLim2, upperLim2, upperLim2, lowerLim2));
lines(c(1.10,1.40), rep(median(vocjack$relatednessrel.morphTypeop.vocjack),2), lwd=2);
lines(c(1.60,1.90), rep(median(vocjack$relatednessrel.morphTypetr.vocjack),2), lwd=2);
points(1.25, fixef(proficiencyglmer6)[13], col=rgb(1,0,0,.8), pch=19, cex=2);
points(1.75, fixef(proficiencyglmer6)[14], col=rgb(1,0,0,.8), pch=19, cex=2);
text(1.25, upperLim1+.5, 'p=.001');
text(1.75, upperLim2+1.5, 'p<.001');

par(mfrow=c(1,1));

```


## Summary of results for the proficiency analysis
We assessed which proficiency score, if any, interacted specifically with prime relatedness and morphological condition. It turned out that this happens for:

- *phonemic discrimination*: [$\chi^2$ = `r round(proficiencyglmer2.anova$Chisq[9], 2)`, $Df$ = `r proficiencyglmer2.anova$Df[9]`, *p*  $<$ 0.0001]. Orthographic priming is significantly different from both opaque, [$\beta$ = `r round(summary(proficiencyglmer2)$coefficients,3)[13,1]`, $Std.Error$ = `r round(summary(proficiencyglmer2)$coefficients,3)[13,2]`, $t$ = `r round(summary(proficiencyglmer2)$coefficients,3)[13,3]`, *p*  $<$ 0.0001], and transparent priming, [$\beta$ = `r round(summary(proficiencyglmer2)$coefficients,3)[14,1]`, $Std.Error$ = `r round(summary(proficiencyglmer2)$coefficients,3)[14,2]`, $t$ = `r round(summary(proficiencyglmer2)$coefficients,3)[14,3]`, *p*  = 0.001], while the latter two conditions do not differ, [$\beta$ = `r round(summary(proficiencyglmer2b)$coefficients,3)[14,1]`, $Std.Error$ = `r round(summary(proficiencyglmer2b)$coefficients,3)[14,2]`, $t$ = `r round(summary(proficiencyglmer2b)$coefficients,3)[14,3]`, *p*  = 0.45].

- *vocabulary*: [$\chi^2$ = `r round(proficiencyglmer6.anova$Chisq[9], 2)`, $Df$ = `r proficiencyglmer6.anova$Df[9]`, *p*  $<$ 0.0001]. Orthographic priming is significantly different from both opaque, [$\beta$ = `r round(summary(proficiencyglmer6)$coefficients,3)[13,1]`, $Std.Error$ = `r round(summary(proficiencyglmer6)$coefficients,3)[13,2]`, $t$ = `r round(summary(proficiencyglmer6)$coefficients,3)[13,3]`, *p*  = 0.001], and transparent priming, [$\beta$ = `r round(summary(proficiencyglmer6)$coefficients,3)[14,1]`, $Std.Error$ = `r round(summary(proficiencyglmer6)$coefficients,3)[14,2]`, $t$ = `r round(summary(proficiencyglmer6)$coefficients,3)[14,3]`, *p*   $<$ 0.0001]. The latter two conditions also differ, [$\beta$ = `r round(summary(proficiencyglmer6b)$coefficients,3)[14,1]`, $Std.Error$ = `r round(summary(proficiencyglmer6b)$coefficients,3)[14,2]`, $t$ = `r round(summary(proficiencyglmer6b)$coefficients,3)[14,3]`, *p*  = 0.0001].

- *morphological awareness*: [$\chi^2$ = `r round(proficiencyglmer3.anova$Chisq[9], 2)`, $Df$ = `r proficiencyglmer3.anova$Df[9]`, *p*  $<$ 0.0001]. Orthographic priming is significantly different from opaque, [$\beta$ = `r round(summary(proficiencyglmer3)$coefficients,3)[13,1]`, $Std.Error$ = `r round(summary(proficiencyglmer3)$coefficients,3)[13,2]`, $t$ = `r round(summary(proficiencyglmer3)$coefficients,3)[13,3]`, *p*  $<$ 0.0001], but *not* transparent priming, [$\beta$ = `r round(summary(proficiencyglmer3)$coefficients,3)[14,1]`, $Std.Error$ = `r round(summary(proficiencyglmer3)$coefficients,3)[14,2]`, $t$ = `r round(summary(proficiencyglmer3)$coefficients,3)[14,3]`, *p*   = 0.32]. The latter two conditions also differ, [$\beta$ = `r round(summary(proficiencyglmer3b)$coefficients,3)[14,1]`, $Std.Error$ = `r round(summary(proficiencyglmer3b)$coefficients,3)[14,2]`, $t$ = `r round(summary(proficiencyglmer3b)$coefficients,3)[14,3]`, *p*  $<$ 0.0001].

The remaining tests -- phonemic fluency, spelling, oral comprehension and reading comprehension— did not reach significance. Respectively: [$\chi^2$ = `r round(proficiencyglmer1.anova$Chisq[9], 2)`, $Df$ = `r proficiencyglmer1.anova$Df[9]`, *p*  = 0.16], [$\chi^2$ = `r round(proficiencyglmer4.anova$Chisq[9], 2)`, $Df$ = `r proficiencyglmer4.anova$Df[9]`, *p*  = 0.47], [$\chi^2$ = `r round(proficiencyglmer7.anova$Chisq[9], 2)`, $Df$ = `r proficiencyglmer7.anova$Df[9]`, *p*  = 0.69], [$\chi^2$ = `r round(proficiencyglmer5.anova$Chisq[9], 2)`, $Df$ = `r proficiencyglmer5.anova$Df[9]`, *p*  = 0.09].


# AoA 

## Correlation
Correlation between the individual scores, and between aoa and proficiency:
```{r}
knitr::kable(round(cor(pptFeatures[,c(13,14,17)], use='pairwise.complete.obs', method='spearman'), digits=2)); 
knitr::kable(round(cor(pptFeatures[,c(6:12, 13)], use='pairwise.complete.obs', method='spearman'), digits=2)[8,]);
```

## Modelling

Make sure the baseline is set to the "orthographic" condition:
```{r set contrast aoa dataEng}
dataEng$morphType <- relevel(dataEng$morphType, "or");
```

We assess whether any AoA metric provides an increse in goodness of fit.
```{r aoaglmer1, eval=FALSE}
aoaglmer1 <- glmer(rt ~ relatedness * morphType*aoa1.Aoa + freqTarget + lengthTarget + (1|subject) + (1|target), data= dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
```
```{r}
anova(proficiencyglmer0, aoaglmer1)->goodness.aoa1
goodness.aoa1

```

```{r aoaglmer2, eval=FALSE}
aoaglmer2 <- glmer(rt ~ relatedness * morphType*aoa2.usage + freqTarget + lengthTarget + (1|subject) + (1|target), data= dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
```
```{r}
anova(proficiencyglmer0,aoaglmer2)->goodness.aoa2
goodness.aoa2
```

```{r aoaglmer3, eval=FALSE}
aoaglmer3 <- glmer(rt ~ relatedness * morphType*aoa3.context + freqTarget + lengthTarget + (1|subject) + (1|target), data= dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
```
```{r}
anova(proficiencyglmer0, aoaglmer3)->goodness.aoa3
goodness.aoa3
```

```{r aoaglmer4, eval=FALSE}
aoaglmer4 <- glmer(rt ~ relatedness * morphType*aoa4.contextMultling + freqTarget + lengthTarget + (1|subject) + (1|target), data= dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
```
```{r}
anova(proficiencyglmer0, aoaglmer4)->goodness.aoa4
goodness.aoa4
```

```{r aoaglmer5, eval=FALSE}
aoaglmer5 <- glmer(rt ~ relatedness * morphType*aoa5.selfRatedProf + freqTarget + lengthTarget + (1|subject) + (1|target), data= dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
```
```{r}
anova(proficiencyglmer0, aoaglmer5)->goodness.aoa5
goodness.aoa5
```

```{r aoaglmer6, eval=FALSE}
aoaglmer6 <- glmer(rt ~ relatedness * morphType*aoa6.otherLang + freqTarget + lengthTarget + (1|subject) + (1|target), data= dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
```
```{r}
anova(proficiencyglmer0, aoaglmer6)->goodness.aoa6
goodness.aoa6
```

It does not seem so, except for AoA5 (which is self-rated proficiency, so this confirms the objective-metrics proficiency analyses). Most notably, AoA proper doesn't contribute any increase in goodness of fit.
Let see whether AoA5 specifically modulate priming:
```{r}
car::Anova(aoaglmer5)-> aoaglmer5.anova
aoaglmer5.anova
```
They do, good. Let see how they do so:
```{r}
knitr::kable(round(summary(aoaglmer5)$coefficients,3));
```
```{r}
dataEng$morphType <- relevel(dataEng$morphType, "op");
```

```{r, eval=FALSE}
aoaglmer5c<- glmer(rt ~ relatedness * morphType*aoa5.selfRatedProf + freqTarget + (1|subject) + (1|target), data= dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
```

```{r}
knitr::kable(round(summary(aoaglmer5c)$coefficients,3));
```

# Summary of the results for the AoA and priming
We first assessed whether *AoA proper* allows an overall better account of RTs. This does not seem to be the case, [$\chi^2$ = `r round(goodness.aoa1$Chisq[2], 2)`, $Df$ = `r goodness.aoa1$Df[2]`, p = .8].

Among the other scores that we collected via the AoA questionnaire, only *self-rated proficiency* improves the quality of the model predictions [$\chi^2$ = `r round(goodness.aoa5$Chisq[2], 2)`, $Df$ = `r goodness.aoa5$Df[2]`, p = .006]. Self-rated proficiency also modulates priming: [$\chi^2$ = `r round(aoaglmer5.anova$Chisq,2)[9]`, $Df$ = `r aoaglmer5.anova$Df[9]`, p $<$ .0001]. Orthographic priming shrinks with growing proficiency significantly more than both opaque, [$\beta$ = `r round(summary(aoaglmer5)$coefficients,2)[13,1]`, $Std.Error$ = `r round(summary(aoaglmer5)$coefficients,2)[13,2]`, $t$ = `r round(summary(aoaglmer5)$coefficients,2)[13,3]`, *p*  $<$ .0001], and transparent priming [$\beta$ = `r round(summary(aoaglmer5)$coefficients,2)[14,1]`, $Std.Error$ = `r round(summary(aoaglmer5)$coefficients,2)[14,2]`, $t$ = `r round(summary(aoaglmer5)$coefficients,2)[14,3]`, *p*  $<$ .0001]. While there is no difference between the latter two, [$\beta$ = `r round(summary(aoaglmer5c)$coefficients,2)[13,1]`, $Std.Error$ = `r round(summary(aoaglmer5c)$coefficients,2)[13,2]`, $t$ = `r round(summary(aoaglmer5c)$coefficients,2)[13,3]`, *p*  = .86]. 


The remaining variables do not affect RTs. 

- *AoA - daily usage*: [$\chi^2$ = `r round(goodness.aoa2$Chisq[2], 2)`, $Df$ = `r goodness.aoa2$Df[2]`, p = .19] 
- *AoA - Home vs School*: [$\chi^2$ = `r round(goodness.aoa3$Chisq[2], 2)`, $Df$ = `r goodness.aoa3$Df[2]`, p = 1]
- *AoA - Multilingual context*: [$\chi^2$ = `r round(goodness.aoa4$Chisq[2], 2)`, $Df$ = `r goodness.aoa4$Df[2]`, p = 1]
- *AoA - other languages*: [$\chi^2$ = `r round(goodness.aoa6$Chisq[2], 2)`, $Df$ = `r goodness.aoa6$Df[2]`, p = 1]


# OSC

First, let's try to pit OSC against priming condition -- these two are typically confounded:

```{r means oscTaget by morphtype}
temp <- unique(masterFile[masterFile$lexicality=='word' & masterFile$language=='eng',c('target','prime','morphType','relatedness','freqTarget','freqPrime','lengthTarget','lengthPrime','nTarget','nPrime','oscTarget')]);
```
```{r}
aggregate(oscTarget ~ morphType, FUN=fivenum, data=temp)

```

Indeed they are.
This tests it more formally:
```{r NHST test}
summary(aov(oscTarget~morphType, data=subset(temp, relatedness=='rel')));
```

## modelling
```{r osc3 glmer, eval=FALSE}
osc2 <- glmer(rt ~ relatedness *  oscTarget * phonemicComprehension + freqTarget + lengthTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
```
```{r}
osc2.anova <- car::Anova(osc2);

```

```{r osc4 glmer, eval=FALSE}
osc3 <- glmer(rt ~ relatedness *  oscTarget * morphComprehension + freqTarget + lengthTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
```
```{r}
osc3.anova <- car::Anova(osc3);

```

```{r osc2 glmer, eval=FALSE}
osc6 <- glmer(rt ~ relatedness *  oscTarget * vocabulary + freqTarget + lengthTarget + (1|subject) + (1|target), data = dataEng, family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
```
```{r}
osc6.anova <- car::Anova(osc6);
```

Morph awareness and vocabulary seem to work:
```{r osc comparisons glmer, eval=FALSE}
knitr::kable(round(summary(osc3)$coefficients,3));
knitr::kable(round(summary(osc6)$coefficients,3));
```

```{r osc6 plot, fig.height = 4, fig.width = 10, fig.align= "center", echo=FALSE}
temp <- data.frame(effect('relatedness:oscTarget:vocabulary', osc6, se=list(level=.95), xlevels=list(oscTarget=c(.20,.80), vocabulary=quantile(dataEng$vocabulary, probs=c(.05,.50,.95)))));
revalue(temp$relatedness, c("rel"="Related"))-> temp$relatedness;
revalue(temp$relatedness, c("ctrl"="Unrelated"))-> temp$relatedness;

vocabulary_names <- c(
  "11" = "Low Vocabulary",
  "16" = "Medium Vocabulary",
  "19" = "High Vocabulary"
);

temp$oscTarget <- as.factor(temp$oscTarget);
ggplot(data = temp, aes(x=relatedness, y=fit, group=oscTarget)) + 
  geom_point(position = position_dodge(width = 0.25)) +
  geom_line(aes(linetype = oscTarget), position = position_dodge(width = 0.25)) + 
  scale_linetype_manual(values=c("dashed", "solid")) +
  theme_bw() + 
  theme(panel.grid.major = element_blank()) +
  ylab('RTs (ms)') + xlab('') + 
  theme(axis.text.y = element_text(angle = 00, hjust = 1, size=8, colour = 'black'))+
  theme(axis.text.x = element_text(size=13, colour = 'black'))+
  geom_pointrange(aes(ymin = lower, ymax = upper), position = position_dodge(width = 0.25)) +
  facet_grid(~ vocabulary, 
             labeller = labeller(vocabulary = as_labeller(vocabulary_names))) +
  theme(strip.text = element_text(size=12))+
  theme(legend.position="none");

```

```{r osc3 plot, fig.height = 4, fig.width = 10, fig.align= "center", echo=FALSE}
temp <- data.frame(effect('relatedness:oscTarget:morphComprehension', osc3, se=list(level=.95), xlevels=list(oscTarget=c(.20,.80), morphComprehension=quantile(dataEng$morphComprehension, probs=c(.05,.50,.95)))));
revalue(temp$relatedness, c("rel"="Related"))-> temp$relatedness;
revalue(temp$relatedness, c("ctrl"="Unrelated"))-> temp$relatedness;

morphComprehension_names <- c(
  "6" = "Low morphAwareness",
  "9" = "Medium morphAwareness",
  "10" = "High morphAwareness"
);

temp$oscTarget <- as.factor(temp$oscTarget);
ggplot(data = temp, aes(x=relatedness, y=fit, group=oscTarget)) + 
  geom_point(position = position_dodge(width = 0.25)) +
  geom_line(aes(linetype = oscTarget), position = position_dodge(width = 0.25)) + 
  scale_linetype_manual(values=c("dashed", "solid")) +
  theme_bw() + 
  theme(panel.grid.major = element_blank()) +
  ylab('RTs (ms)') + xlab('') + 
  theme(axis.text.y = element_text(angle = 00, hjust = 1, size=8, colour = 'black'))+
  theme(axis.text.x = element_text(size=13, colour = 'black'))+
  geom_pointrange(aes(ymin = lower, ymax = upper), position = position_dodge(width = 0.25)) +
  facet_grid(~ morphComprehension, 
             labeller = labeller(morphComprehension = as_labeller(morphComprehension_names))) +
  theme(strip.text = element_text(size=12))+
  theme(legend.position="none");


```

## OSC in unrelated trials

We have checked in the models here that OSC also has a main effect, and whether OSC has a main effect and/or interact with any of the proficiency metric when you consider RTs *only to unrelated trials*.

```{r osc1_unrelatedTrials, eval=FALSE}
osc1_unrelatedTrials <- glmer(rt ~ oscTarget * phonemicFluency + freqTarget + lengthTarget + (1|subject) + (1|target), data = dataEng[dataEng$relatedness=="ctrl",], family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));

```
```{r}
osc1_unrelatedTrials.anova <- car::Anova(osc1_unrelatedTrials)
```

```{r osc2_unrelatedTrials, eval=FALSE}
osc2_unrelatedTrials <- glmer(rt ~  oscTarget * phonemicComprehension + freqTarget + lengthTarget + (1|subject) + (1|target), data = dataEng[dataEng$relatedness=="ctrl",], family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));

```
```{r}
osc2_unrelatedTrials.anova <- car::Anova(osc2_unrelatedTrials)
```

```{r osc3_unrelatedTrials, eval=FALSE}
osc3_unrelatedTrials <- glmer(rt ~  oscTarget * morphComprehension + freqTarget + lengthTarget + (1|subject) + (1|target), data = dataEng[dataEng$relatedness=="ctrl",], family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));

```

```{r}
osc3_unrelatedTrials.anova <-car::Anova(osc3_unrelatedTrials)
```

```{r osc4_unrelatedTrials, eval=FALSE}
osc4_unrelatedTrials <- glmer(rt ~  oscTarget * spelling + freqTarget + lengthTarget + (1|subject) + (1|target), data = dataEng[dataEng$relatedness=="ctrl",], family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));

```

```{r}
osc4_unrelatedTrials.anova <-car::Anova(osc4_unrelatedTrials)
```

```{r osc5_unrelatedTrials, eval=FALSE}
osc5_unrelatedTrials <- glmer(rt ~  oscTarget * readingComprehension + freqTarget + lengthTarget + (1|subject) + (1|target), data = dataEng[dataEng$relatedness=="ctrl",], family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));
```

```{r}
osc5_unrelatedTrials.anova <-car::Anova(osc5_unrelatedTrials)
```

```{r osc6_unrelatedTrials, eval=FALSE}
osc6_unrelatedTrials <- glmer(rt ~ oscTarget * vocabulary + freqTarget + lengthTarget + (1|subject) + (1|target), data = dataEng[dataEng$relatedness=="ctrl",], family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));

```
```{r}
osc6_unrelatedTrials.anova <-car::Anova(osc6_unrelatedTrials)
```

```{r osc7_unrelatedTrials, eval=FALSE}
osc7_unrelatedTrials <- glmer(rt ~ oscTarget * oralComprehension + freqTarget + lengthTarget + (1|subject) + (1|target), data = dataEng[dataEng$relatedness=="ctrl",], family=Gamma(link="identity"), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)));

```
```{r}
osc7_unrelatedTrials.anova <-car::Anova(osc7_unrelatedTrials)
```

```{r}
osc2_unrelatedTrials.anova
osc3_unrelatedTrials.anova
```

This checks whether OSC provides a better account for the data as compared to the more classic category distinction, as suggested by Reviewer 1:
```{r}
AIC(osc3); AIC(proficiencyglmer3); #this is for morphological awareness
AIC(osc6); AIC(proficiencyglmer6); #this is for vocabulary
```

## Summary of results for OSC by proficiency
we also wanted to assess the role of Orthography--to--Semantics Consistency (OSC) in L2, and particularly whether this variable affects morphological priming.

There is a significant main effect of OSC in:

- *phonemic discrimination:* [$\chi^2$ = `r round(osc2.anova$Chisq,2)[2]`, $Df$ = `r osc2.anova$Df[1]`, p $<$ .0001]

- *morphological awareness:* [$\chi^2$ = `r round(osc3.anova$Chisq,2)[2]`, $Df$ = `r osc3.anova$Df[1]`, p $<$ .0001]

- *vocabulary:* [$\chi^2$ = `r round(osc6.anova$Chisq,2)[1]`, $Df$ = `r osc6.anova$Df[2]`, p $<$ .0001]



Moreover, OSC modulates morphological priming in L2 via interactions with:

- *vocabulary:* [$\chi^2$ = `r round(osc6.anova$Chisq,2)[9]`, $Df$ = `r osc6.anova$Df[9]`, p $<$ .0001]

- *morphological awareness:* [$\chi^2$ = `r round(osc3.anova$Chisq,2)[9]`, $Df$ = `r osc3.anova$Df[9]`, p $<$ .0001]

Phonemic discrimination doesn't modulate priming with OSC, [$\chi^2$ = `r round(osc2.anova$Chisq,2)[9]`, $Df$ = `r osc2.anova$Df[9]`, p = .6]

# Plots  

## Figure 1 - plot of estimated RTs

```{r extract data from the itaglmer2 model and plot it, fig.height = 4, fig.width = 6, fig.align= "center", echo=FALSE}
df <- effect("relatedness:morphType",itaglmer2);
df <- as.data.frame(df);
revalue(df$relatedness, c("ctrl"="Unrelated"))-> df$relatedness;
revalue(df$relatedness, c("rel"="Related"))-> df$relatedness;

dodge1 <- position_dodge(width = 0.25);
bb  <-ggplot(data = df, aes(x = relatedness, y = fit,group = morphType)) +
  geom_point(size = 2, position = dodge1) +
  geom_line(aes(linetype=morphType), position = dodge1) + 
  scale_linetype_manual(values=c("dotted", "dashed", "solid")) +
  theme_classic();
bb  <- bb + geom_pointrange(aes(ymin = df$lower, ymax = df$upper), position = dodge1) ;
bb  <- bb + scale_y_continuous("RT(ms)") ;
bb  <- bb + theme(axis.title.y = element_text(size = rel(1), angle = 90));
bb  <- bb + theme(axis.text.y = element_text(angle = 00, hjust = 1, size=10, colour = 'black'));
bb  <- bb + theme(axis.title.x = element_blank()) + theme(axis.text.x = element_text(size=13, colour = 'black'));
bb <- bb + labs(title='L1 - Italian');
bb <- bb + theme(plot.title= element_text(angle = 00, hjust=0.5, size=15, face = 'bold', colour = 'black'));
bb <- bb + theme(legend.position="none")
bb
```


```{r extract data from the engglmer2 model and plot it, fig.height = 4, fig.width = 6, fig.align= "center", echo=FALSE}
df <- effect("relatedness:morphType",engglmer2); 
df <- as.data.frame(df);
revalue(df$relatedness, c("ctrl"="Unrelated"))-> df$relatedness;
revalue(df$relatedness, c("rel"="Related"))-> df$relatedness;

dodge <- position_dodge(width = 0.25);
gg  <-ggplot(data = df, aes(x = relatedness, y = fit,group = morphType)) + 
  geom_point(size = 2, position = dodge) + 
  geom_line(aes(linetype=morphType), position = dodge) + 
  scale_linetype_manual(values=c( "dotted", "dashed", "solid")) + 
  theme_classic();
gg  <- gg + geom_pointrange(aes(ymin = df$lower, ymax = df$upper), position = dodge);
gg  <- gg + scale_y_continuous("RT (ms)") ;
gg  <- gg + theme(axis.text.y = element_text(angle = 00, hjust = 1, size=10, colour = 'black'));
gg  <- gg + theme(axis.title.x = element_blank()) + theme(axis.text.x = element_text(size=13, colour = 'black'));
gg <- gg + labs(title='L2 - English');
gg <- gg + theme(plot.title= element_text(angle = 00, hjust=0.5, size=15, face = 'bold', colour = 'black'));
gg<- gg + theme(legend.position="none")
gg
```

## Figure 2 - Participants' score distributions for each proficiency subtest

```{r barplots proficiency scores, fig.height = 6, fig.width = 12.5, fig.align= "center", echo=FALSE, warning=FALSE}
#jpeg(filename = paste(localGitDir,'/figure2.jpg', sep = ''), res=300, height=2200, width=4400);
par(mfrow=c(2,4));
par(mar=c(5,5,4,.5)+.1);
par(lwd=2);

CleanEnvir(vocabulary);
attach(pptFeatures);

hist(phonemicFluency, breaks = seq(0,50,5), main = '(a) Phon Fluency', cex.main=2, xlab = 'Scores', ylab = 'N of participants', ylim=c(0,50), cex.lab=2, axes=F, col=grey(.80), border=grey(0));
axis(1, cex.axis=2);
axis(2, at=c(0,50), cex.axis=2, las=1);

hist(phonemicComprehension, breaks = seq(0,13,1), main = '(b) Phon Discrimination', cex.main=2, xlab = 'Scores', ylab = 'N of participants', ylim=c(0,50), cex.lab=2, axes=F, col=grey(.80), border=grey(0));
axis(1, cex.axis=2);
axis(2, at=c(0,50), cex.axis=2, las=1);

hist(morphComprehension, breaks = seq(0,10,1), main = '(c) Morph Awareness', cex.main=2, xlab = 'Scores', ylab = 'N of participants', ylim=c(0,50), cex.lab=2, axes=F, col=grey(.80), border=grey(0));
axis(1, cex.axis=2);
axis(2, at=c(0,50), cex.axis=2, las=1);

hist(spelling, breaks = seq(0,20,2), main = '(d) Spelling', cex.main=2, xlab = 'Scores', ylab = 'N of participants', ylim=c(0,50), cex.lab=2, axes=F, col=grey(.80), border=grey(0));
axis(1, cex.axis=2);
axis(2, at=c(0,50), cex.axis=2, las=1);

hist(readingComprehension, breaks = seq(0,7,1), main = '(e) Read Comprehension', cex.main=2, xlab = 'Scores', ylab = 'N of participants', ylim=c(0,50), cex.lab=2, axes=F, col=grey(.80), border=grey(0));
axis(1, cex.axis=2);
axis(2, at=c(0,50), cex.axis=2, las=1);

hist(vocabulary, breaks = seq(0,20,2), main = '(f) Vocabulary', cex.main=2, xlab = 'Scores', ylab = 'N of participants', ylim=c(0,50), cex.lab=2, axes=F, col=grey(.80), border=grey(0));
axis(1, cex.axis=2);
axis(2, at=c(0,50), cex.axis=2, las=1);

hist(oralComprehension, breaks = seq(0,6,1), main = '(g) Oral comprehension', cex.main=2, xlab = 'Scores', ylab = 'N of participants', ylim=c(0,50), cex.lab=2, axes=F, col=grey(.80), border=grey(0));
axis(1, cex.axis=2);
axis(2, at=c(0,50), cex.axis=2, las=1);
#dev.off()
detach(pptFeatures);
```

```{r close plot, echo=FALSE, warning=FALSE}
par(mfrow=c(1,1));
```


## Figure 3 - Interaction by phonemic discrimination by relatedness and morphtype

```{r phonemicComprehension plot , fig.height = 4, fig.width = 10, fig.align= "center", echo=FALSE}
temp <- data.frame(effect('relatedness:morphType:phonemicComprehension', proficiencyglmer2, se=list(level=.95), xlevels=list(phonemicComprehension=quantile(dataEng$phonemicComprehension, probs=c(.05,.50,.95)))));
revalue(temp$relatedness, c("rel"="Related"))-> temp$relatedness;
revalue(temp$relatedness, c("ctrl"="Unrelated"))-> temp$relatedness;

phonComprehension_names <- c(
  "5" = "Low phonDiscrimination",
  "9" = "Medium phonDiscrimination",
  "12" = "High phonDiscrimination");

ggplot(data = temp, aes(x=relatedness, y=fit, group=morphType)) + 
  geom_point(size = 2, position = position_dodge(width = 0.25)) +
  geom_line(aes(linetype=morphType), position = position_dodge(width = 0.25)) + 
  scale_linetype_manual(values=c("dotted", "dashed", "solid")) +
  theme_bw() + 
  theme(panel.grid.major = element_blank()) +
  ylab('RTs (ms)') + xlab('') + 
  theme(axis.text.y = element_text(angle = 00, hjust = 1, size=8, colour = 'black'))+
  theme(axis.text.x = element_text(size=13, colour = 'black'))+
  geom_pointrange(aes(ymin = lower, ymax = upper), position = position_dodge(width = 0.25)) +
  facet_grid(~ phonemicComprehension, 
             labeller = labeller(phonemicComprehension = as_labeller(phonComprehension_names))) +
  theme(strip.text = element_text(size=12)) + 
  theme(legend.position="none");


```
## Figure 4 - Interaction by vocabulary by relatedness and morphtype

```{r vocabulary plot 2, fig.height = 4, fig.width = 10, fig.align= "center", echo=FALSE}
temp <- data.frame(effect('relatedness:morphType:vocabulary', proficiencyglmer6, se=list(level=.95), xlevels=list(vocabulary=quantile(dataEng$vocabulary, probs=c(.05,.50,.95)))));
revalue(temp$relatedness, c("rel"="Related"))-> temp$relatedness;
revalue(temp$relatedness, c("ctrl"="Unrelated"))-> temp$relatedness;

vocabulary_names <- c(
  "11" = "Low vocabulary",
  "16" = "Medium vocabulary",
  "19" = "High vocabulary");

ggplot(data = temp, aes(x=relatedness, y=fit, group=morphType)) + 
  geom_point(size = 2, position = position_dodge(width = 0.25)) +
  geom_line(aes(linetype=morphType), position = position_dodge(width = 0.25)) + 
  scale_linetype_manual(values=c("dotted", "dashed", "solid")) +
  theme_bw() + 
  theme(panel.grid.major = element_blank()) +
  ylab('RTs (ms)') + xlab('') + 
  theme(axis.text.y = element_text(angle = 00, hjust = 1, size=8, colour = 'black'))+
  theme(axis.text.x = element_text(size=13, colour = 'black'))+
  geom_pointrange(aes(ymin = lower, ymax = upper), position = position_dodge(width = 0.25)) +
  facet_grid(~ vocabulary, 
             labeller = labeller(vocabulary = as_labeller(vocabulary_names))) +
  theme(strip.text = element_text(size=12)) + 
  theme(legend.position="none");

```

## Figure 5 - Interaction by morphological awareness by relatedness and morphtype

```{r morphological awareness plot 2, fig.height = 4, fig.width = 10, fig.align= "center", echo=FALSE}
temp <- data.frame(effect('relatedness:morphType:morphComprehension', proficiencyglmer3, se=list(level=.95), xlevels=list(morphComprehension=quantile(dataEng$morphComprehension, probs=c(.05,.50,.95)))));
revalue(temp$relatedness, c("rel"="Related"))-> temp$relatedness;
revalue(temp$relatedness, c("ctrl"="Unrelated"))-> temp$relatedness;

morphComprehension_names <- c(
  "6" = "Low morphAwareness",
  "9" = "Medium morphAwareness",
  "10" = "High morphAwareness");

ggplot(data = temp, aes(x=relatedness, y=fit, group=morphType)) + 
  geom_point(size = 2, position = position_dodge(width = 0.25)) +
  geom_line(aes(linetype=morphType), position = position_dodge(width = 0.25)) + 
  scale_linetype_manual(values=c("dotted", "dashed", "solid")) +
  theme_bw() + 
  theme(panel.grid.major = element_blank()) +
  ylab('RTs (ms)') + xlab('') + 
  theme(axis.text.y = element_text(angle = 00, hjust = 1, size=8, colour = 'black'))+
  theme(axis.text.x = element_text(size=13, colour = 'black'))+
  geom_pointrange(aes(ymin = lower, ymax = upper), position = position_dodge(width = 0.25)) +
  facet_grid(~ morphComprehension, 
             labeller = labeller(morphComprehension = as_labeller(morphComprehension_names))) +
  theme(strip.text = element_text(size=12)) + 
  theme(legend.position="none");


```

## Figure 6 - jackknife

```{r, echo=FALSE}
group3df<-read.csv(paste0(localGitDir,"/analysis/jaccknife/bootstrap_groupAnalysis.csv"))
phonDiscr<-read.csv(paste0(localGitDir,"/analysis/jaccknife/bootstrap_phonDiscr.csv"))
morphAware<-read.csv(paste0(localGitDir,"/analysis/jaccknife/bootstrap_morphAware.csv"))
vocabulary<-read.csv(paste0(localGitDir,"/analysis/jaccknife/bootstrap_voc.csv"))

par(mfrow=c(2,2));
par(mar=c(1,1,3,.5)+.1);
#par(lwd=2);

plot(c(1,2), c(0,-8), type='n', xlim=c(1,2), ylim=c(-10.5, 1), axes=F, main='Phonemic Discrimination', xlab='', ylab='Beta');
axis(1, at=c(1.25,1.75), labels=c('Opaque-Orthographic', 'Transparent-Orthographic'));
axis(2);
abline(h=0, lty=2, lwd=2);
lowerLim1 <- quantile(phonDiscr$relatednessrel.morphTypeop.phonemicComprehension, probs=.05);
upperLim1 <- quantile(phonDiscr$relatednessrel.morphTypeop.phonemicComprehension, probs=.95);
lowerLim2 <- quantile(phonDiscr$relatednessrel.morphTypetr.phonemicComprehension, probs=.05);
upperLim2 <- quantile(phonDiscr$relatednessrel.morphTypetr.phonemicComprehension, probs=.95);
polygon(c(1.10,1.10,1.40,1.40), c(lowerLim1, upperLim1, upperLim1, lowerLim1));
polygon(c(1.60,1.60,1.90,1.90), c(lowerLim2, upperLim2, upperLim2, lowerLim2));
lines(c(1.10,1.40), rep(median(phonDiscr$relatednessrel.morphTypeop.phonemicComprehension),2), lwd=2);
lines(c(1.60,1.90), rep(median(phonDiscr$relatednessrel.morphTypetr.phonemicComprehension),2), lwd=2);
points(1.25, fixef(proficiencyglmer2)[13], col=rgb(1,0,0,.8), pch=19, cex=2);
points(1.75, fixef(proficiencyglmer2)[14], col=rgb(1,0,0,.8), pch=19, cex=2);
text(1.25, upperLim1+1, 'p<.001');
text(1.75, upperLim2+1, 'p<.001');

plot(c(1,2), c(0,-8), type='n', xlim=c(1,2), ylim=c(-5.5, 10), axes=F, main='Morphological Awareness', xlab='', ylab='Beta');
axis(1, at=c(1.25,1.75), labels=c('Opaque-Orthographic', 'Transparent-Orthographic'));
axis(2);
abline(h=0, lty=2, lwd=2);
lowerLim1 <- quantile(morphAware$relatednessrel.morphTypeop.morphComprehension, probs=.05);
upperLim1 <- quantile(morphAware$relatednessrel.morphTypeop.morphComprehension, probs=.95);
lowerLim2 <- quantile(morphAware$relatednessrel.morphTypetr.morphComprehension, probs=.05);
upperLim2 <- quantile(morphAware$relatednessrel.morphTypetr.morphComprehension, probs=.95);
polygon(c(1.10,1.10,1.40,1.40), c(lowerLim1, upperLim1, upperLim1, lowerLim1));
polygon(c(1.60,1.60,1.90,1.90), c(lowerLim2, upperLim2, upperLim2, lowerLim2));
lines(c(1.10,1.40), rep(median(morphAware$relatednessrel.morphTypeop.morphComprehension)-.03,2), lwd=2);
lines(c(1.60,1.90), rep(median(morphAware$relatednessrel.morphTypetr.morphComprehension),2), lwd=2);
points(1.25, fixef(proficiencyglmer3)[13], col=rgb(1,0,0,.8), pch=19, cex=2);
points(1.75, fixef(proficiencyglmer3)[14], col=rgb(1,0,0,.8), pch=19, cex=2);
text(1.25, upperLim1+1, 'p<.001');
text(1.75, upperLim2+1, 'p=.311');

plot(c(1,2), c(0,-8), type='n', xlim=c(1,2), ylim=c(-7.5, 3), axes=F, main='Vocabulary', xlab='', ylab='Beta');
axis(1, at=c(1.25,1.75), labels=c('Opaque-Orthographic', 'Transparent-Orthographic'));
axis(2);
abline(h=0, lty=2, lwd=2);
lowerLim1 <- quantile(vocabulary$relatednessrel.morphTypeop.vocabulary, probs=.05);
upperLim1 <- quantile(vocabulary$relatednessrel.morphTypeop.vocabulary, probs=.95);
lowerLim2 <- quantile(vocabulary$relatednessrel.morphTypetr.vocabulary, probs=.05);
upperLim2 <- quantile(vocabulary$relatednessrel.morphTypetr.vocabulary, probs=.95);
polygon(c(1.10,1.10,1.40,1.40), c(lowerLim1, upperLim1, upperLim1, lowerLim1));
polygon(c(1.60,1.60,1.90,1.90), c(lowerLim2, upperLim2, upperLim2, lowerLim2));
lines(c(1.10,1.40), rep(median(vocabulary$relatednessrel.morphTypeop.vocabulary),2), lwd=2);
lines(c(1.60,1.90), rep(median(vocabulary$relatednessrel.morphTypetr.vocabulary),2), lwd=2);
points(1.25, fixef(proficiencyglmer6)[13], col=rgb(1,0,0,.8), pch=19, cex=2);
points(1.75, fixef(proficiencyglmer6)[14], col=rgb(1,0,0,.8), pch=19, cex=2);
text(1.25, upperLim1+.5, 'p=.001');
text(1.75, upperLim2+1.5, 'p<.001');

plot(c(1,2), c(8,-31), type='n', xlim=c(1,2), ylim=c(-35, 10), axes=F, main='Group-level analysis', xlab='', ylab='Beta');
axis(1, at=c(1.25,1.75), labels=c('Opaque-Orthographic', 'Transparent-Orthographic'));
axis(2);
abline(h=0, lty=2, lwd=2);
lowerLim1 <- quantile(group3df$relatednessrel.morphTypeop, probs=.05);
upperLim1 <- quantile(group3df$relatednessrel.morphTypeop, probs=.95);
lowerLim2 <- quantile(group3df$relatednessrel.morphTypetr, probs=.05);
upperLim2 <- quantile(group3df$relatednessrel.morphTypetr, probs=.95);
polygon(c(1.10,1.10,1.40,1.40), c(lowerLim1, upperLim1, upperLim1, lowerLim1));
polygon(c(1.60,1.60,1.90,1.90), c(lowerLim2, upperLim2, upperLim2, lowerLim2));
lines(c(1.10,1.40), rep(median(group3df$relatednessrel.morphTypeop),2), lwd=2);
lines(c(1.60,1.90), rep(median(group3df$relatednessrel.morphTypetr),2), lwd=2);
points(1.25, fixef(engglmer2)[7], col=rgb(1,0,0,.8), pch=19, cex=2);
points(1.75, fixef(engglmer2)[8], col=rgb(1,0,0,.8), pch=19, cex=2);
text(1.25, 5, 'p=.059');
text(1.75, -7.5, 'p<.001');

par(mfrow=c(1,1));
```


## Figure 7 - Scores distributions in the AoA questionnaire

```{r aoa distribution, fig.height = 6, fig.width = 12.5, fig.align= "center", echo=FALSE}
#jpeg(filename = paste(localGitDir,'/aoaScores.jpg', sep = ''), res=300, height=2200, width=4400); 
par(mfrow=c(2,3));
par(mar=c(5,5,4,.5)+.1);
par(lwd=2);

attach(pptFeatures);

hist(aoa1.Aoa, breaks = seq(-.5,15.5,1), main = '(a) Age first exposed', cex.main=2, xlab = 'Scores', ylab = 'N of participants', ylim=c(0,30), cex.lab=2, axes=F, col=grey(.80), border=grey(0));
axis(1, cex.axis=2);
axis(2, at=c(0,30), cex.axis=2, las=1);

hist(aoa2.usage, breaks = seq(.5,5.5,1), main = '(b) Daily use', cex.main=2, xlab = 'Scores', ylab = 'N of participants', ylim=c(0,30), cex.lab=2, axes=F, col=grey(.80), border=grey(0));
axis(1, cex.axis=2);
axis(2, at=c(0,30), cex.axis=2, las=1);

barplot(table(aoa3.context), main = '(c) Where did you learn?', cex.main=2,  ylab = 'N of participants', ylim=c(0,65), cex.lab=2, cex.names=2, axes=F, col=grey(.80), border=grey(0));
axis(2, at=c(0,65), cex.axis=2, las=1);

barplot(table(aoa4.contextMultling), main = '(d) Multilingual context', cex.main=2, ylab = 'N of participants', ylim=c(0,65), cex.lab=2, cex.names=2, axes=F, col=grey(.80), border=grey(0));
axis(2, at=c(0,65), cex.axis=2, las=1);

hist(aoa5.selfRatedProf, breaks = seq(.5,5.5,1), main = '(e) Self rated proficiency', cex.main=2, xlab = 'Scores', ylab = 'N of participants', ylim=c(0,30), cex.lab=2, axes=F, col=grey(.80), border=grey(0));
axis(1, cex.axis=2);
axis(2, at=c(0,30), cex.axis=2, las=1);

barplot(table(aoa6.otherLang), main = '(f) Additional languages?', cex.main=2, ylab = 'N of participants', ylim=c(0,65), cex.lab=2, cex.names=2, axes=F, col=grey(.80), border=grey(0));
axis(2, at=c(0,65), cex.axis=2, las=1);

detach(pptFeatures);

par(mfrow=c(1,1));
#dev.off();

```

## Figure 8 - OSC boxplot distribution by morphtype

```{r OSC boxplot, fig.height = 4, fig.width = 6, fig.align= "center", echo=FALSE}
temp <- unique(masterFile[masterFile$lexicality=='word' & masterFile$language=='eng',c('target','prime','morphType','relatedness','freqTarget','freqPrime','lengthTarget','lengthPrime','nTarget','nPrime','oscTarget')]);

revalue(temp$morphType, c("or"="Orthographic", 'op'='Opaque', 'tr'='Transparent'))-> temp$morphType;
library(ggpubr);
ggboxplot(subset(temp, oscTarget>0), "morphType", "oscTarget",
          color = "black", fill = grey(.80),
          width = 0.5, ylab = 'OSC', xlab = ''); 

```

## Figure 9 - Osc and vocabulary

```{r osc6, fig.height = 4, fig.width = 10, fig.align= "center", echo=FALSE}
temp <- data.frame(effect('relatedness:oscTarget:vocabulary', osc6, se=list(level=.95), xlevels=list(oscTarget=c(.20,.80), vocabulary=quantile(dataEng$vocabulary, probs=c(.05,.50,.95)))));
revalue(temp$relatedness, c("rel"="Related"))-> temp$relatedness;
revalue(temp$relatedness, c("ctrl"="Unrelated"))-> temp$relatedness;

vocabulary_names <- c(
  "11" = "Low Vocabulary",
  "16" = "Medium Vocabulary",
  "19" = "High Vocabulary"
);

temp$oscTarget <- as.factor(temp$oscTarget);
ggplot(data = temp, aes(x=relatedness, y=fit, group=oscTarget)) + 
  geom_point(position = position_dodge(width = 0.25)) +
  geom_line(aes(linetype = oscTarget), position = position_dodge(width = 0.25)) + 
  scale_linetype_manual(values=c("dashed", "solid")) +
  theme_bw() + 
  theme(panel.grid.major = element_blank()) +
  ylab('RTs (ms)') + xlab('') + 
  theme(axis.text.y = element_text(angle = 00, hjust = 1, size=8, colour = 'black'))+
  theme(axis.text.x = element_text(size=13, colour = 'black'))+
  geom_pointrange(aes(ymin = lower, ymax = upper), position = position_dodge(width = 0.25)) +
  facet_grid(~ vocabulary, 
             labeller = labeller(vocabulary = as_labeller(vocabulary_names))) +
  theme(strip.text = element_text(size=12))+
  theme(legend.position="none");

```

## Figure 10 - OSC and morphological awareness

```{r osc3, fig.height = 4, fig.width = 10, fig.align= "center", echo=FALSE}
temp <- data.frame(effect('relatedness:oscTarget:morphComprehension', osc3, se=list(level=.95), xlevels=list(oscTarget=c(.20,.80), morphComprehension=quantile(dataEng$morphComprehension, probs=c(.05,.50,.95)))));
revalue(temp$relatedness, c("rel"="Related"))-> temp$relatedness;
revalue(temp$relatedness, c("ctrl"="Unrelated"))-> temp$relatedness;

morphComprehension_names <- c(
  "6" = "Low morphAwareness",
  "9" = "Medium morphAwareness",
  "10" = "High morphAwareness"
);

temp$oscTarget <- as.factor(temp$oscTarget);
ggplot(data = temp, aes(x=relatedness, y=fit, group=oscTarget)) + 
  geom_point(position = position_dodge(width = 0.25)) +
  geom_line(aes(linetype = oscTarget), position = position_dodge(width = 0.25)) + 
  scale_linetype_manual(values=c("dashed", "solid")) +
  theme_bw() + 
  theme(panel.grid.major = element_blank()) +
  ylab('RTs (ms)') + xlab('') + 
  theme(axis.text.y = element_text(angle = 00, hjust = 1, size=8, colour = 'black'))+
  theme(axis.text.x = element_text(size=13, colour = 'black'))+
  geom_pointrange(aes(ymin = lower, ymax = upper), position = position_dodge(width = 0.25)) +
  facet_grid(~ morphComprehension, 
             labeller = labeller(morphComprehension = as_labeller(morphComprehension_names))) +
  theme(strip.text = element_text(size=12))+
  theme(legend.position="none");


```




