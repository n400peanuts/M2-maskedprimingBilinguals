tan(3.14*1/180)*70
sin(3.14*45/180)
cos(3.14*45/180)
cos(3.14*0/180)
cos(3.14*90/180)
sin(3.14*90/180)
sin(3.14*0/180)
tan(3.14*16/180)*1047
cotan(3.14*16/180)*(598/2)
atan(3.14*16/180)*(598/2)
install.packages("pracma")
library(pracma)
ctan(3.14*16/180)*(598/2)
cotan(3.14*16/180)*(598/2)
cot(3.14*16/180)*(598/2)
tan(3.14*.25/180)*c(676,852,929,1047)
2.95/.301
3.72/.253
4.05/.277
4.57/.311
tan(3.14*.35/180)*c(676,852,929,1047)
6.39/.311
5.67/.277
5.20/.253
4.12/.301
32/.25
32/.35
a <- sample(100,1:1000)
b <- sample(100,1:1000)
a
a <- sample(1:1000, 100)
b <- sample(1:1000, 100)
a
b
cor(a,b)
cor(b,a)
lm(a ~ b)
lm(b ~ a)
summary(lm(b ~ a))
summary(lm(a ~ b))
anova(lm(b ~ a))
anova(lm(a ~ b))
.01/.78
library("lmerTest", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
NeuralNetwork_OneWord_Training <- function(in.act, target, nhidden, nepochs, learningrate)
{
#Input-to-hidden connections; set random from -1 to 1
in.hidden <<- matrix( rnorm( length(in.act)*nhidden, mean=0, sd=.35), nrow=length(in.act));
#Hidden-to-output connections; set random from -1 to 1
hidden.out <<- matrix( rnorm( length(target)*nhidden, mean=0, sd=.35), nrow=nhidden);
#Backward output-to-hidden connections; set random from -1 to 1
out.hidden <<- matrix( rnorm( length(target)*nhidden, mean=0, sd=.20), nrow=length(target));
#Recurrent hidden layer connections; set random from -1 to 1
hidden.hidden <<- matrix( rnorm( nhidden^2, mean=0, sd=.20), nrow=nhidden);
#Bias nodes for hidden and output layers; fixed activation of 1
biasout.act <- 1;
biashidden.act <- 1;
#Bias nodes to hidden layer; set initially to -1; vertical dimension is time
biasout.out <<- rep(-1, length(target));
biashidden.hidden <<- rep(-1, nhidden);
#Initializes hidden- and output-layer node activations; vertical dimension is time
hidden.net <- matrix(rep(NA, nhidden*nepochs), nrow=nepochs, byrow=T);
hidden.act <<- matrix(c(rep(0.5, nhidden), rep(NA,nhidden*(nepochs-1))), nrow=nepochs, byrow=T);
out.net <- matrix(rep(NA, length(target)*nepochs), nrow=nepochs, byrow=T);
out.act <<- matrix(c(rep(0, length(target)), rep(NA,length(target)*(nepochs-1))), nrow=nepochs, byrow=T);
#Initializes the divergence error metric
error <<- rep(NA, nepochs);
#Time starts here
for (time in 2:nepochs)
{
#Computes hidden-layer node activations [learning phase -> here the output-to-hidden feedback is based on the target, not on the actual activations across the output layer]
hidden.net[time,] <- in.act %*% in.hidden + biashidden.act * biashidden.hidden + target %*% out.hidden + hidden.act[time-1,] %*% hidden.hidden;
hidden.act[time,] <<- 1 / (1 + (exp(-hidden.net[time,])));
#Computes output-layer node activations
out.net[time,] <- hidden.act[time,] %*% hidden.out + biasout.act * biasout.out;
out.act[time,] <<- exp(out.net[time,]) / sum(exp(out.net[time,]));
#Evaluates the performance [this is different from Botvinick & Plaut (2006): they used logs, which gave me problems, so I switched to classical total quadratic error]
#error[time] <<- sum(target * log( (target/out.act[time,]) + rnorm(ncol(in.matrix), mean=0, sd=.01) )); [this is what, I guess, was used by Botvinick & Plaut (2006)
error[time] <<- .5 * sum((target - out.act[time,])^2) / length(in.act);
#error[time, -item.order[time-1]] <<- error[time-1, -item.order[time-1]];
############################## Start of Learning ##############################
#[Not sure whether this is what Botvinick & Plaut (2006) used, they do not specify this in great details in their paper. I applied the standard backpropagation algorithm]
#Computes delta for output units
out.delta <- target - out.act[time,];
#Computes delta for hidden units [this is where the backpropagation algorithm is applied]
hidden.delta <- rep(0,nhidden);
for (i in 1:length(in.act))
{
temp <- out.delta[i] * ( abs(hidden.out[,i]) / sum(abs(hidden.out[,i])) );
hidden.delta <- hidden.delta + temp;
};
#Computes the change matrix for input-to-hidden nodes [here I simply apply the Delta rule]
in.hidden.change <- learningrate * ( t(matrix(in.act, nrow=1)) %*% hidden.delta);
#Computes the change matrix for hidden-to-output nodes
hidden.out.change <- learningrate * ( t(matrix(hidden.act[time,], nrow=1)) %*% out.delta);
#Computes the change matrix for feedback output-to-hidden nodes
out.hidden.change <- learningrate * ( t(matrix(out.act[time,], nrow=1)) %*% hidden.delta);
#Computes the change matrix for recursive hidden-to-hidden nodes
hidden.hidden.change <- learningrate * ( t(matrix(hidden.act[time,], nrow=1)) %*% hidden.delta);
#Computes the change matrix for bias-to-hidden nodes
biashidden.hidden.change <- learningrate * (biashidden.act * hidden.delta);
#Computes the change matrix for bias-to-output nodes
biasout.out.change <- learningrate * (biasout.act * out.delta);
#Apply changes
hidden.out <<- hidden.out + hidden.out.change;
in.hidden <<- in.hidden + in.hidden.change;
out.hidden <<- out.hidden + out.hidden.change;
hidden.hidden <<- hidden.hidden + hidden.hidden.change;
biashidden.hidden <<- biashidden.hidden + biashidden.hidden.change;
biasout.out <<- biasout.out + biasout.out.change;
############################## End of Learning ##############################
#Visualizes the cycle number and the net accuracy dinamically
#I don't want it to display output at all cycles, let's do it every 100 epochs
if (as.integer(time/100) == time/100 | time == nepochs)
{
par(mfrow = c(1,3));
barplot(time, col="blue", main="Cycle number", xlab="", ylab="Cycle number", ylim=c(0,nepochs));
barplot(error[time], col="red", ylab="Mean square error", xlab="", main="Accuracy", ylim=c(0, max(error[], na.rm=T)));
barplot(matrix(c(target, out.act[time,]), nrow=2, byrow=T), beside=T, col=c("red","blue"), xlab="Output nodes", names.arg=seq(1:length(in.act)), ylab="Activation", main="Red is target, blue is actual value");
};
};
############################## Start of Visualization ##############################
#Draws the canvas
jpeg("OneWordTraining.LearningThroughEpochs.jpg", res=200, width=2339, height=1654);
layout(matrix(c(1,1,1,2,3,4,1,1,1,seq(5,19,1)), 4, 6, byrow = TRUE));
#Draws the squared error by epoch
plot(error, type="b", pch=19, xlab="Epochs", ylab="Mean square error per output node", main="Accuracy through epochs");
#Compares target vs. actual activation at the output layer
step <- as.integer(nepochs/18);
for (i in 1:18) barplot(matrix(c(target, out.act[i*step,]), nrow=2, byrow=T), beside=T, col=c("red","blue"), xlab="Output nodes", names.arg=seq(1:length(in.act)), ylab="Activation", main=paste("Epoch:",i*step));
dev.off();
################################ End of Visualization ################################
}
NeuralNetwork_OneWord_Training(c(0,1,1,0), c(1,0,0,1), 4, 10, .01)
NeuralNetwork_OneWord_Training(c(0,1,1,0), c(1,0,0,1), 2, 1000, .01)
NeuralNetwork_OneWord_Training(c(0,1,1,0), c(1,0,0,1), 2, 1000, .1)
NeuralNetwork_OneWord_Training(c(0,1,1,0), c(1,0,0,1), 3, 1000, .1)
NeuralNetwork_OneWord_Training(c(0,1,1,0), c(1,0,0,1), 3, 100000, .1)
NeuralNetwork_OneWord_Training(c(0,1,1,0), c(1,0,0,1), 3, 10000, .01)
NeuralNetwork_OneWord_Training(c(0,1,1,0), c(0,1,1,0), 3, 10000, .01)
##################################### Start of function to running simulations of single words on the trained newtork #####################################
NeuralNetwork_OneWord_Running <- function(in.act, target)
{
#Bias nodes for hidden and output layers; fixed activation of 1
run_biasout.act <- 1;
run_biashidden.act <- 1;
#Initializes hidden- and output-layer node activations; vertical dimension is time
run_hidden.net <- rep(NA, ncol(in.hidden));
run_hidden.act <<- rep(NA, ncol(in.hidden));
run_simout.net <- rep(NA, ncol(hidden.out));
run_out.act <<- rep(NA, ncol(hidden.out));
#Initializes the divergence error metric
run_error <<- NA;
#The network starts working
#Computes hidden-layer node activations from input activations and bias node
run_hidden.net <- in.act %*% in.hidden + run_biashidden.act * biashidden.hidden;
run_hidden.act <<- 1 / (1 + (exp(-run_hidden.net)));
#Computes output-layer node activations from hidden activations and bias node
run_out.net <- run_hidden.act %*% hidden.out + run_biasout.act * biasout.out;
run_out.act <<- exp(run_out.net) / sum(exp(run_out.net));
#Now the network should adjust hidden-layer activation through backward connection from the output layer and though reverberating connections from the hidden layer itself; but we're simulating single words here, so don't need this.
#Evaluates the performance [this is different from Botvinick & Plaut (2006): they used logs, which gave me problems, so I switched to classical total quadratic error]
run_error <<- .5 * sum((target - run_out.act)^2) / length(in.act);
#Draws a graph comparing actual to target activations across the output layer
jpeg("OneWordRunnin.Performance.jpg", res=200, width=1669, height=827);
barplot(matrix(c(target, run_out.act), nrow=2, byrow=T), beside=T, col=c("red","blue"), xlab="Output nodes", names.arg=seq(1:length(in.act)), ylab="Activation", main="Red is target, blue is actual value");
dev.off();
};
##################################### End of function to running simulations of single words on the trained newtork #####################################
NeuralNetwork_OneWord_Running(c(0,1,1,0), c(0,1,1,0))
rm(list=ls());
subtlex.us <- read.table("~/Documents/ResearchTools/Frequenzari/Subtlex-US/SUBTLEXus74286wordstextversion.txt", header=T);
temp1 <- subtlex.us[grep("pa", subtlex.us$Word),];
temp2 <- subtlex.us[grep("aa", subtlex.us$Word),];
temp3 <- subtlex.us[grep("ak", subtlex.us$Word),];
head(temp1)
sum(nrow(temp1),nrow(temp2),nrow(temp3)) -> paaktype;
#token frequency
sum(sum(temp1$FREQcount),sum(temp2$FREQcount),sum(temp3$FREQcount)) -> paaktoken;
#poad
temp1 <- subtlex.us[grep("po", subtlex.us$Word),];
temp2 <- subtlex.us[grep("oa", subtlex.us$Word),];
temp3 <- subtlex.us[grep("ad", subtlex.us$Word),];
#type frequency
sum(nrow(temp1),nrow(temp2),nrow(temp3)) -> poadtype;
#token frequency
sum(sum(temp1$FREQcount),sum(temp2$FREQcount),sum(temp3$FREQcount)) -> poadtoken;
par(mfrow=c(2,1));
barplot(c(paaktype, poadtype), beside=T, names.arg=c("paak","poad"));
barplot(c(paaktoken, poadtoken), beside=T, names.arg=c("paak","poad"));
par(mfrow=c(2,1));
barplot(c(paaktype, poadtype), beside=T, names.arg=c("paak","poad"));
par(mfrow=c(1,1));
barplot(c(paaktype, poadtype), beside=T, names.arg=c("paak","poad"));
temp1 <- subtlex.us[grep("pa", subtlex.us$Word),];
temp2 <- subtlex.us[grep("aa", subtlex.us$Word),];
temp3 <- subtlex.us[grep("ak", subtlex.us$Word),];
nrow(temp2)
sum(temp2$FREQcount)
temp2 <- subtlex.us[grep("oa", subtlex.us$Word),];
nrow(temp2)
sum(temp2$FREQcount)
barplot(c(paaktoken, poadtoken), beside=T, names.arg=c("paak","poad"));
temp1 <- subtlex.us[grep("p", subtlex.us$Word),];
temp2 <- subtlex.us[grep("pa", subtlex.us$Word),];
temp3 <- subtlex.us[grep("a", subtlex.us$Word),];
temp4 <- subtlex.us[grep("aa", subtlex.us$Word),];
temp5 <- subtlex.us[grep("a", subtlex.us$Word),];
temp6 <- subtlex.us[grep("ak", subtlex.us$Word),];
#trans probs
sum(temp2$FREQcount) / sum(temp1$FREQcount)-> agivenp;
sum(temp4$FREQcount) / sum(temp3$FREQcount)-> agivena;
sum(temp6$FREQcount) / sum(temp5$FREQcount)-> kgivena;
#poad
temp1 <- subtlex.us[grep("p", subtlex.us$Word),];
temp2 <- subtlex.us[grep("po", subtlex.us$Word),];
temp3 <- subtlex.us[grep("o", subtlex.us$Word),];
temp4 <- subtlex.us[grep("oa", subtlex.us$Word),];
temp5 <- subtlex.us[grep("a", subtlex.us$Word),];
temp6 <- subtlex.us[grep("ad", subtlex.us$Word),];
#trans probs
sum(temp2$FREQcount) / sum(temp1$FREQcount)-> ogivenp;
sum(temp4$FREQcount) / sum(temp3$FREQcount)-> agiveno;
sum(temp6$FREQcount) / sum(temp5$FREQcount)-> dgivena;
barplot(c(agivenp, agivena, kgivena, sum(agivenp, agivena, kgivena)), beside=T, names.arg=c("t1","t2","t3","sum"), ylim=c(0,.2), main="paak");
barplot(c(ogivenp, agiveno, dgivena, sum(ogivenp, agiveno, dgivena)), beside=T, names.arg=c("t1","t2","t3","sum"), ylim=c(0,.2), main="poad");
par(mfrow=c(2,1));
barplot(c(agivenp, agivena, kgivena, sum(agivenp, agivena, kgivena)), beside=T, names.arg=c("t1","t2","t3","sum"), ylim=c(0,.2), main="paak");
barplot(c(ogivenp, agiveno, dgivena, sum(ogivenp, agiveno, dgivena)), beside=T, names.arg=c("t1","t2","t3","sum"), ylim=c(0,.2), main="poad");
barplot(c(agivenp, agivena, kgivena, sum(agivenp, agivena, kgivena)), beside=T, names.arg=c("t1","t2","t3","sum"), ylim=c(0,.2), main="paak");
par(mfrow=c(1,1));
barplot(c(agivenp, agivena, kgivena, sum(agivenp, agivena, kgivena)), beside=T, names.arg=c("t1","t2","t3","sum"), ylim=c(0,.2), main="paak");
barplot(c(ogivenp, agiveno, dgivena, sum(ogivenp, agiveno, dgivena)), beside=T, names.arg=c("t1","t2","t3","sum"), ylim=c(0,.2), main="poad");
1/144
1/144*c(1,2,3,4,5,6,7,8,9,10)
round(1/144*c(1,2,3,4,5,6,7,8,9,10), digits=3)
1/144
1/144*c(5,6,7,8)
install.packages("lsr")
install.packages("ES")
e
log(500)
log(500,10)
log(550)
-1000/log(550)
-1000*log(550)
324/5
(625*5*40)/1000
10*1000/60
8*1000/60
8*500/60
(625*5*40)/250
8*250/60
6*250/60
5*250/60
6*250/60
6*300/60
(625*5*40)/300
load("~/Documents/Ricerca/Morphology/MorphoOrthographicItalian/Experiments/IMPnew/relatedPrimes/data20140313/DataForAnalysis.RData")
View(data.rt)
60*2*41*
60*2*41
x <- rnorm(1000, mean=0, sd=1)
x
plot(x)
plot(density(x))
sd(x)
mean(x)
sd(x)/ (sqrt(2)/pi)
pi
sqrt(2)
sqrt(2)/pi
sx(x)
sd(x)
1.01376/.4501582
c(NA,NA,6,7,8) -> georgie
georgie
mean(georgie)
mean(georgie[is.na(georgie)])
is.na(georgie)
!is.na(georgie)
mean(georgie[!is.na(georgie)])
library("corrplot", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("rms", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
load("~/Documents/Ricerca/Morphology/BimbiNormali-Daniela&Marco/dataLastVersion/giugno2014.RData")
summary(dat3)
summary(dat4)
tapply((dat4$RT),list(dat4$type,dat4$rel,dat4$Classe),mean)
round(tapply((dat4$RT),list(dat4$type,dat4$rel,dat4$Classe),mean), digits=0)
round(tapply((dat4$RT),list(dat4$type,dat4$rel),mean), digits=0)
x <- seq(-10,10,.01)
plot(x, -(x^2 + x + 1))
x <- seq(1,5,1)
plot(x, -(x^2 + x + 1))
plot(x, -(x^2 + x + 4))
plot(x, -(x^2 + x + 1))
x <- seq(1,5,.01)
plot(x, -(x^2 + x + 1))
plot(x, -(4*x^2 + x + 1))
plot(x, -(x^2 + x))
plot(x, -(x-3)^2)
plot(x, -2(x-3)^2)
plot(x, -2*(x-3)^2)
plot(x, -(x-3)^2)
plot(x, -(x-8)^2)
plot(x, -(x-3)^2)
lines(x, -10(x-3)^2)
lines(x, -10*(x-3)^2)
subtlex <- read.table('~/Documents/ResearchTools/Frequenzari/subtlex-it.txt', header=T)
head(subtlex)
subset(subtlex, spelling=='pollo')
library(vwr);
coltheart.N('pollo', as.character(subtlex$spelling));
stimuli <- c('pippo','pluto','paperino');
coltheart.N(stimuli, as.character(subtlex$spelling));
subset(subtlex, length==5 & zipf>4.40 & zipf<4.60)
coltheart.N('ricco', as.character(subtlex$spelling));
subset(subtlex, spelling=='fiore')
subset(subtlex, spelling=='fiorista')
coltheart.N(c('fiore','fiorista'), as.character(subtlex$spelling));
subset(subtlex, length==8 & zipf>2.60 & zipf<2.90 & substr(as.character(spelling), nchar(as.character(spelling))-2, nchar(as.character(spelling)))=='ore')
subset(subtlex, length==8 & zipf>2.40 & zipf<3.10 & substr(as.character(spelling), nchar(as.character(spelling))-2, nchar(as.character(spelling)))=='ore')
subset(subtlex, length==8 & zipf>2.40 & zipf<3.10 & substr(as.character(spelling), nchar(as.character(spelling))-2, nchar(as.character(spelling)))=='ista')
subset(subtlex, length==8 & zipf>2.40 & zipf<3.10 & substr(as.character(spelling), nchar(as.character(spelling))-2, nchar(as.character(spelling)))=='isto')
subset(subtlex, length==8 & zipf>2.40 & zipf<3.10 & substr(as.character(spelling), nchar(as.character(spelling))-3, nchar(as.character(spelling)))=='ista')
coltheart.N(c('questore','fiorista'), as.character(subtlex$spelling));
subset(subtlex, spelling=='costa')
subset(subtlex, spelling=='costanza')
coltheart.N(c('costa','costanza'), as.character(subtlex$spelling));
coltheart.N(c('postino','costanza'), as.character(subtlex$spelling));
subset(subtlex, spelling=='postino')
subset(subtlex, spelling=='bagnino')
subset(subtlex, spelling %in% c('trama','corda','pinna','tappo'))
subset(subtlex, spelling %in% c('cordoglio'))
coltheart.N(c('corda','cordoglio'), as.character(subtlex$spelling));
subset(subtlex, spelling %in% c('giubbotto'))
subset(subtlex, length==9 & zipf>2.70 & zipf<2.80)
subset(subtlex, length==9 & zipf>2.70 & zipf<2.80)
subset(subtlex, length==9 & zipf>2.70 & zipf<2.80)$spelling
subset(subtlex, spelling %in% c('discordia'))
coltheart.N(c('corda','cordoglio'), as.character(subtlex$spelling));
coltheart.N(c('corda','discordia'), as.character(subtlex$spelling));
pippo <- read.delim('~/Documents/ResearchTools/Frequenzari/subtlex-it.txt', header=T)
head(pippo)
library(vwr)
install.packages("retimes")
library(retimes
)
load("~/Documents/Ricerca/Morphology/MorphoOrthographicItalian/Experiments/IMPnew/relatedPrimes/data20140313/DataForAnalysis.RData")
head(data.rt)
mexgauss(data.rt$rt[data.rt$F2_requested_duration==.058333])
mexgauss(data.rt$rt[data.rt$F2_requested_duration==.033333])
levels(data.rt$F2_requested_duration)
mexgauss(data.rt$rt[data.rt$ticks==1])
mexgauss(data.rt$rt[data.rt$ticks==2])
mexgauss(data.rt$rt[data.rt$ticks==3])
mexgauss(data.rt$rt[data.rt$ticks==4])
plot(rexgauss(1000, mu=0, sigma=1, tau=1), type="l)"
plot(rexgauss(1000, mu=0, sigma=1, tau=1), type="l")
rexgauss(10, mu=0, sigma=1, tau=1)
rexgauss(10o, mu=0, sigma=1, tau=1)
rexgauss(100, mu=0, sigma=1, tau=1)
mexgauss(data.rt$rt[data.rt$ticks==1])
mexgauss(data.rt$rt[data.rt$ticks==2])
mexgauss(data.rt$rt[data.rt$ticks==3])
mexgauss(data.rt$rt[data.rt$ticks==4])
mexgauss(data.rt$rt[data.rt$ticks==5])
mexgauss(data.rt$rt[data.rt$ticks==6])
mexgauss(data.rt$rt[data.rt$ticks==7])
mexgauss(data.rt$rt[data.rt$ticks==8])
install.packages("pROC")
library(pROC)
roc(response=c(0,0,0,1,1,1), predictor=c(0,0,1,0,1,1))
plot(roc(response=c(0,0,0,1,1,1), predictor=c(0,0,1,0,1,1)))
plot(roc(response=c(0,0,0,1,1,1), predictor=c(0,0,1,0,1,1), smooth=T))
plot(roc(response=c(0,0,0,1,1,1), predictor=c(0,0,1,0,1,1)))
plot(roc(response=c(0,0,0,1,1,1), predictor=c(0,1,1,0,0,1)))
plot(roc(response=c(0,0,0,1,1,1), predictor=c(1,1,1,0,0,0)))
rbinom(50,.50)
sample(c(0,1),2)
sample(c(0,1),100)
plot(roc(response=, predictor=c(rep(1,100),rep(0,100))))
sample(c(rep(1,1000),rep(0,1000)),200)
plot(roc(response=sample(c(rep(1,1000),rep(0,1000)),200), predictor=c(rep(1,100),rep(0,100))))
plot(roc(response=sample(c(rep(1,1000),rep(0,1000)),200), predictor=c(rep(1,100),rep(0,100))))
plot(roc(response=c(rep(1,100),rep(0,100)), predictor=c(rep(1,100),rep(0,100))))
install.packages("psyphy")
c(15,8)/c(30,28)
c(1,6)/c(30,28)
c(6,1)/c(30,28)
matrix(c(15,8,1,6,0,5,6,1,3,9,0,8,9,1), nrow=7, byrow=T)
round( matrix(c(15,8,1,6,0,5,6,1,3,9,0,8,9,1), nrow=7, byrow=T) / matrix(rep(c(30,28),6), nrow=7, byrow=T)
round( matrix(c(15,8,1,6,0,5,6,1,3,9,0,8,9,1), nrow=7, byrow=T) / matrix(rep(c(30,28),6), nrow=7, byrow=T)
, digits=2)
round( matrix(c(15,8,1,6,0,5,6,1,3,9,0,8,9,1), nrow=7, byrow=T) / matrix(rep(c(30,28),7), nrow=7, byrow=T)
, digits=2)
round( matrix(c(15,8,1,6,0,5,6,1,3,9,0,8,9,1), nrow=7, byrow=T) / matrix(rep(c(30,28),7), nrow=7, byrow=T), digits=2) -> temp
temp[,3] <- abs(temp[,1]-temp[,2])
temp[,1]
temp[,2]
abs(temp[,1]-temp[,2])
pchisq(matrix(c(0,30,5,23), nrow=2))
matrix(c(0,30,5,23)
matrix(c(0,30,5,23), nrow=2)
matrix(c(0,30,5,23), nrow=2, byrow=T)
matrix(c(30,0,23,5), nrow=2, byrow=T)
pchisq(matrix(c(30,0,23,5), nrow=2, byrow=T))
chisq.test(matrix(c(30,0,23,5), nrow=2, byrow=T), simulate.p.value=T)
chisq.test(matrix(c(15,15,20,8), nrow=2, byrow=T), simulate.p.value=T)
742+592+65+69+700+20+800+500
742+592+65+69+519+20+800+500
14500/.40
14500*.40
4900/15000
742+592+65+69+519+20+800+500
subtlex <- read.table("~/DriveAtNeuroMi/resources/subtlex-it.txt", header=T)
head(subtlex)
subset(subtlex, spelling=="cremisi")
subset(subtlex, spelling=="giallognolo")
subset(subtlex, spelling=="sbriluccicante")
quantile(subtlex$zipf)
subtlex <- read.table('~/DriveAtNeuroMi/resources/subtlex-it.txt', header=T)
summary(subtlex)
pippo <- read.table('~/DriveAtNeuroMi/resources/subtlex-it.txt', header=T)
head(pippo)
subset(pippo, grepl('ero', pippo$spelling))
subset(pippo, substr(as.character(pippo$spelling), nchar(as.character(pippo$spelling))-2, nchar(as.character(pippo$spelling)) )=='ero')
subset(pippo, substr(as.character(pippo$spelling), nchar(as.character(pippo$spelling))-2, nchar(as.character(pippo$spelling)) )=='bero')
subset(pippo, substr(as.character(pippo$spelling), nchar(as.character(pippo$spelling))-3, nchar(as.character(pippo$spelling)) )=='bero')
subset(pippo, substr(as.character(pippo$spelling), nchar(as.character(pippo$spelling))-3, nchar(as.character(pippo$spelling)) )=='bero') -> pluto
head(pluto)
head(pluto, n=50)
head(pluto, n=100)
head(pluto, n=200)
pluto$spelling
subset(pippo, substr(as.character(pippo$spelling), nchar(as.character(pippo$spelling))-3, nchar(as.character(pippo$spelling)) )=='pero') -> pluto
pluto
1498000*.20
34*3
34*4*2
34*3.5*2
8000/30
setwd('~/DriveAtNeuroMi/research/misc/m2-maskedMorphPrimingBilinguals/git/M2-maskedprimingBilinguals/')
masterFile <- read.table('~/DriveAtNeuroMi/research/misc/m2-maskedMorphPrimingBilinguals/git/M2-maskedprimingBilinguals/data/masterFile.txt', header=T)
source('bilingualDiagnostics.R')
subset(masterFile, Language=="ita")-> masterfileIta
sbj.id<- masterfileIta$Subject
acc<- masterfileIta$Accuracy
lexicality<- masterfileIta$Lexicality
target<-masterfileIta$Target
rt<-masterfileIta$rt
source("Diagnostics.R")
diagnostics.f(rt, acc, sbj.id, target, lexicality, ita)
subset(masterFile, Language=="ita")-> masterfileIta
sbj.id<- masterfileIta$Subject
acc<- masterfileIta$Accuracy
lexicality<- masterfileIta$Lexicality
target<-masterfileIta$Target
rt<-masterfileIta$rt
rm(list=ls())
subset(masterFile, Language=="ita")-> masterfileIta
masterFile <- read.table('~/DriveAtNeuroMi/research/misc/m2-maskedMorphPrimingBilinguals/git/M2-maskedprimingBilinguals/data/masterFile.txt', header=T)
subset(masterFile, Language=="ita")-> masterfileIta
sbj.id<- masterfileIta$Subject
acc<- masterfileIta$Accuracy
lexicality<- masterfileIta$Lexicality
target<-masterfileIta$Target
rt<-masterfileIta$rt
source("Diagnostics.R")
diagnostics.f(rt, acc, sbj.id, target, lexicality, ita)
subset(masterfileIta, masterfileIta$rt>250 & masterfileIta$rt<1600 & masterfileIta$Subject!=16 & masterfileIta$Target!= "congruo" & masterfileIta$Target!= "guado" & masterfileIta$Target!= "guano" & masterfileIta$Target!= "uggia" & masterfileIta$Target!= "vello" & masterfileIta$Target!= "avo" & masterfileIta$Lexicality=="WORD") -> dataAccITA
subset(dataAccITA, dataAccITA$Accuracy==1)-> datartITA
summary(datartITA)
round(xtabs(datartITA$rt ~ datartITA$Morphtype + datartITA$Primetype) / xtabs(~datartITA$Morphtype + datartITA$Primetype), digits = 0)
subset(masterfileEng, masterfileEng$rt>250 & masterfileEng$rt<1900 & masterfileEng$Subject!=22 & masterfileEng$Subject!=26 & masterfileEng$Lexicality=="WORD") -> dataAccENG
#Then, we select only right answers
subset(dataAccENG, dataAccENG$Accuracy==1)-> datartENG
subset(masterFile, Language=="eng")-> masterfileEng
subset(masterfileEng, masterfileEng$rt>250 & masterfileEng$rt<1900 & masterfileEng$Subject!=22 & masterfileEng$Subject!=26 & masterfileEng$Lexicality=="WORD") -> dataAccENG
subset(dataAccENG, dataAccENG$Accuracy==1)-> datartENG
round(xtabs(datartENG$rt ~ datartENG$Morphtype + datartENG$Primetype) / xtabs(~datartENG$Morphtype + datartENG$Primetype), digits = 0)
rm(rt, acc, sbj.id, target, lexicality, diagnostics.f, masterFile);
attach(datartITA)
par(mfrow=c(1,3))
qqnorm(rt)
qqnorm(log(rt))
qqnorm(-1000/rt)
par(mfrow=c(1,1))
detach(datartITA)
italmer5b <- lmer(-1000/rt ~ Relatedness * Morphtype + Logfreq.Zipf.t + (1|Subject) + (1|Target), data= subset(datartITA, abs(scale(resid(italmer5)))<2), REML = F)
library(lmerTest)
italmer5b <- lmer(-1000/rt ~ Relatedness * Morphtype + Logfreq.Zipf.t + (1|Subject) + (1|Target), data= subset(datartITA, abs(scale(resid(italmer5)))<2), REML = F)
datartITA$Morphtype <- relevel(datartITA$Morphtype, "OR")
italmer5 <- lmer(-1000/rt ~ Relatedness * Morphtype + Logfreq.Zipf.t + (1|Subject) + (1|Target), data= datartITA, REML = F)
italmer5b <- lmer(-1000/rt ~ Relatedness * Morphtype + Logfreq.Zipf.t + (1|Subject) + (1|Target), data= subset(datartITA, abs(scale(resid(italmer5)))<2), REML = F)
anova(italmer5b)
library(languageR)
inv <- function(x) { -1000/x}
plotLMER.fnc(italmer5b, fun = inv, pred = Relatedness, intr = list(Morphtype, c("OR", "OP", "TR"), "end"), addlines = T)
plotLMER.fnc(italmer5b, fun = inv, pred = Relatedness, intr = list("Morphtype", c("OR", "OP", "TR"), "end"), addlines = T)
plotLMER.fnc(italmer5b, fun = inv, pred = "Relatedness", intr = list("Morphtype", c("OR", "OP", "TR"), "end"), addlines = T)
englmer5 <- lmer(-1000/rt ~ Logfreq.Zipf.t + (1|Subject) + (1|Target), data= datartENG, REML = F)
datartENG$Morphtype <- relevel(datartENG$Morphtype, "OR")
englmer6 <- lmer(-1000/rt ~ Relatedness * Morphtype + Logfreq.Zipf.t + Lent +(1|Subject) + (1|Target), data= datartENG, REML = F)
englmer6b <- lmer(-1000/rt ~ Relatedness * Morphtype + Logfreq.Zipf.t + Lent + (1|Subject) + (1|Target), data= subset(datartENG, abs(scale(resid(englmer6)))<2), REML = F)
anova(englmer6b)
plotLMER.fnc(englmer6b, fun = inv, pred = "Relatedness", intr = list("Morphtype", c("OR", "OP", "TR"), "end"), addlines = T)
summary(datartITA)
proficiencyData <- datartENG[,c('Subject','Age','Gender','Handedness','Rotation','phoneticFluency','morpgComprehension','spelling','readingComprehension','vocabulary','oralComprehension','AoA1', 'AoA2', 'AoA3','AoA4','AoA5','AoA6','AoA7','AoA8','AoA9')];
summary(datartENG)
proficiencyData <- datartENG[,c('Subject','Age','Gender','Handedness','Rotation','phoneticFluency','morphComprehension','spelling','readingComprehension','vocabulary','oralComprehension','AoA1', 'AoA2', 'AoA3','AoA4','AoA5','AoA6','AoA7','AoA8','AoA9')];
proficiencyData <- unique(proficiencyData);
